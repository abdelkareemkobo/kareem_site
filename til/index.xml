<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>ÿπÿ®ÿØ ÿßŸÑŸÉÿ±ŸäŸÖ ÿßŸÑÿÆÿ∑Ÿäÿ® - Abdelkareem Elkhateb | AI Researcher &amp; Arabic NLP Specialist</title>
<link>https://kareemai.com/til/</link>
<atom:link href="https://kareemai.com/til/index.xml" rel="self" type="application/rss+xml"/>
<description>Latest TILs by Kareem</description>
<image>
<url>https://user-images.githubusercontent.com/1483922/208359430-f55d7503-3a98-4875-a35c-16314c9439d0.png</url>
<title>ÿπÿ®ÿØ ÿßŸÑŸÉÿ±ŸäŸÖ ÿßŸÑÿÆÿ∑Ÿäÿ® - Abdelkareem Elkhateb | AI Researcher &amp; Arabic NLP Specialist</title>
<link>https://kareemai.com/til/</link>
</image>
<generator>quarto-1.8.26</generator>
<lastBuildDate>Sat, 13 Dec 2025 22:00:00 GMT</lastBuildDate>
<item>
  <title>Rust for DataScience 2026 - TIL</title>
  <dc:creator>kareem </dc:creator>
  <link>https://kareemai.com/til/tils/2025-12-14.html</link>
  <description><![CDATA[ 





<section id="rust-for-data-science" class="level2">
<h2 class="anchored" data-anchor-id="rust-for-data-science">Rust for Data science</h2>
<p>Yesterday, I had and idea about creating a keyword spell checker for Arabic and I was trying to contribute to ÿßŸÑŸÖÿπŸÑŸÖ ÿßŸÑŸÇÿ±ÿßŸÜÿ¶ project which needs some preprocessing pipelines and multiple projects that i am very curious how it‚Äôs built in AI and some of them are : 1. Polars : it‚Äôs very efficient and interesting project I use it daily 2. Qdrant: my main vector database 3. Fast-palid : multivector late interaction indexing built with rust And the list goes from inference engine for static embedding and much more.</p>
</section>
<section id="ÿ±ÿ≥ÿ™-ŸÑŸÑÿ∫ŸÑÿßÿ®ÿ©---ÿ£ÿ≠ŸÖÿØ-ŸÅÿ±ÿ∫ŸÑ" class="level2">
<h2 class="anchored" data-anchor-id="ÿ±ÿ≥ÿ™-ŸÑŸÑÿ∫ŸÑÿßÿ®ÿ©---ÿ£ÿ≠ŸÖÿØ-ŸÅÿ±ÿ∫ŸÑ">ÿ±ÿ≥ÿ™ ŸÑŸÑÿ∫ŸÑÿßÿ®ÿ© - ÿ£ÿ≠ŸÖÿØ ŸÅÿ±ÿ∫ŸÑ</h2>
<p>I found Ahmed plalist about rust in Arabic speakers it‚Äôs streaming videos around 3 hours each. I started with the first one because I tried to learn rust two times before and didn‚Äôt understand it and I read the book 3 years ago.</p>
<p>The Introduction and the way of explaining is very nice and cool one. I really love the idea of series : it‚Äôs not just rust as language but Rust with programming thinking about how to design and Operating system works.. it makes the concepts a lot easier and practical. I loved how it compare rust features to other languages and how he explained how each concept affect other concept. In the first video he has explaining how something like</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode rust code-with-copy"><code class="sourceCode rust"><span id="cb1-1">Let Data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb1-2">    a<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">u8</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb1-3">    b<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">i32</span></span>
<span id="cb1-4"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div></div>
<p>What is the align , and their size and how it‚Äôs added to memory in efficient way and why this is different from C language ..etc Why the order doesn‚Äôt matter for rust compiler. How memory is working like heap, stack , system calls , word, bytes how the compiler see them and use precompile, the explaining of why macro in rust exist and a lot of concepts I encountered before was very hard to me now they makes sense.</p>
<p>I will continue the series it‚Äôs rare one and the good thing he teach in Arabic ‚≠ê‚≠ê‚≠ê ### Rust Project for ML I plan to do the following projects after I get some knowledge for rust 1. Better Docs for Rust and Python with code embedded search with static embedding model 1. The rust doc is amazing compared to the poor python documentation it‚Äôs very bad and ugly one! 2. Lialia : Arabic keywords checker with Lexical search 3. Bm25 with Rust 1. BMX from mixeadbread in Rust 4. Pyversity in Rust 5. Semhash + Rust + Qdrant 6. RSS reader with DSPY 7. Mgrep in Rust</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ol type="1">
<li><a href="https://youtube.com/playlist?list=PLald6EODoOJU0GMuYHlkS9MLhTPE7HiaT&amp;si=3O-9VHNwmSN5M13t">ÿ±ÿ≥ÿ™ ŸÑŸÑÿ∫ŸÑÿßÿ®ÿ©</a></li>
</ol>


</section>

 ]]></description>
  <category>blogging</category>
  <category>til</category>
  <guid>https://kareemai.com/til/tils/2025-12-14.html</guid>
  <pubDate>Sat, 13 Dec 2025 22:00:00 GMT</pubDate>
  <media:content url="https://kareemai.com/til/tils/til.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Revisit the Basics of Arabic NLP</title>
  <dc:creator>kareem </dc:creator>
  <link>https://kareemai.com/til/tils/2025-12-13.html</link>
  <description><![CDATA[ 





<ul>
<li>When I was a student I was learning about classical machine learning and NLP I encountered a lot of concept like:</li>
<li>TF-ID</li>
<li>Bm25 and other concept I skimmed them and get the idea but I didn‚Äôt apply it into Arabic with practical code after these years</li>
<li>I was reading about the following topics</li>
</ul>
<ol type="1">
<li>Lexical Embedding</li>
<li>MiniCol from Qdrant</li>
<li>StaticEmbedding (minislab and sbert) I understand the new advanced that are related to Bert and Transformer but the basics about sparse embedding are totally missing like how they are using n-gram to subsample for a large space then train an MLP to simulate a dense embedding to create a faster model what are the parameters of this architecture how to optimize it‚Ä¶etc all these knowledge are not connect well. So I want to revisit these concepts again in roadmap</li>
</ol>
<ul>
<li>word2vec</li>
<li>n-gram</li>
<li>stemming</li>
<li>TF-IDF</li>
<li>BM25
<ul>
<li>BMX</li>
</ul></li>
<li>Sparse embedding
<ul>
<li>splade</li>
<li>train a sparse embedding with sbert This is my initial thoughts but I think a better way it to start reading this book and try to implement the code for Arabic NLP: Speech and Language Processing An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models Third Edition draft Daniel Jurafsky Stanford University James H. Martin University of Colorado at Boulder</li>
</ul></li>
</ul>
<section id="chapter-summary-and-what-to-do-next" class="level2">
<h2 class="anchored" data-anchor-id="chapter-summary-and-what-to-do-next">Chapter Summary and what to do next</h2>
<ul>
<li>the chapter was an introduction about words, tokens , BPE and tokenization and some linguistics stuff, and how is the Unicode and encoding for different language is done which was nice intro.</li>
<li>How to use regular expression to do some NLP analysis.</li>
<li>what I really like most was the Minimum Edit Distance and Alignment of words. <strong>The problem I had is implement the algorhtim yourself needs some problem solving and I didn‚Äôt practice problem solving for a long time and my skills is no there yet</strong> **** I tried for 30 minutes to implement minimum distance but failed. I opened a website for leetcode problems is more organized and minimal called <a href="neetcode.com">neetcode</a> I solved the first 11 problems 3 was very easy, and other were medium and the good thing that I had solutions that are not in the solutions sections and my focus on how to use python collections methods like counter is good. The code solves the problem but it‚Äôs not the fastest thing. But I feel good that my patience and thinking is better. ![[Screenshot_2025-12-14-10-35-27-07_e4424258c8b8649f6e67d283a50a2cbc.jpg]]</li>
<li>my month plan will be to finish all the 150 problem and do a lot of dynamic programming (DP) because the minimum edit distance and viterbi in the end of the first chapter will be implemented in DP and other things I know also needs DP.</li>
<li>After that i will start practice on the following alternative to leetcode but for machine learning <strong>deep-ml.com</strong> ![[Pasted image 20251214122627.png]] ### Build a Keyword checker in Arabic I get a nice idea while learning about alignment that I could build my own keyword spell checker for my language that is more smart than the current keyboards by creating the following features:</li>
</ul>
<ol type="1">
<li>Recommend technical meaning
<ol type="1">
<li>If I said word like embedding model it can suggest =&gt; ŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿ™ÿ∂ŸÖŸäŸÜ</li>
</ol></li>
<li>Recommend keywords related to my persona (engineer or doctor..etc)</li>
<li>Federated Learning Keyboard like gboard</li>
<li>Arabic Grammars corrected</li>
<li>Automatic learning based on user interactions I thought it will be nice even to implement this in rust :) ü¶Äü¶Ä ### References</li>
<li><a href="https://web.stanford.edu/~jurafsky/slp3/">The NLP book</a></li>
</ol>


</section>

 ]]></description>
  <category>blogging</category>
  <category>til</category>
  <guid>https://kareemai.com/til/tils/2025-12-13.html</guid>
  <pubDate>Fri, 12 Dec 2025 22:00:00 GMT</pubDate>
  <media:content url="https://kareemai.com/til/tils/til.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Evaluating Arabic Tokenizers</title>
  <dc:creator>kareem </dc:creator>
  <link>https://kareemai.com/til/tils/2025-10-21.html</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>When working with Arabic language models, choosing the right tokenizer can significantly impact model performance and efficiency. In this post, I‚Äôll share my experience comparing two popular Arabic tokenizers: <strong>AraModernBert</strong> and <strong>AraBERT v2</strong>.</p>
</section>
<section id="why-tokenizer-evaluation-matters" class="level2">
<h2 class="anchored" data-anchor-id="why-tokenizer-evaluation-matters">Why Tokenizer Evaluation Matters</h2>
<p>After reading the comprehensive guide on <a href="https://www.fast.ai/posts/2025-10-16-karpathy-tokenizers.html#token-efficiency">GPT tokenizers</a>, I realized that tokenization is often overlooked but critically important. Poor tokenization can lead to:</p>
<ul>
<li>Inefficient use of limited context windows</li>
<li>Higher computational costs (you pay per token!)</li>
<li>Worse model performance, especially for non-English languages</li>
</ul>
<p>For Arabic specifically, tokenization is challenging because:</p>
<ul>
<li>Arabic has rich morphology with prefixes and suffixes</li>
<li>Different dialects (Egyptian, Levantine, Gulf) have varying vocabulary</li>
<li>Diacritical marks (tashkeel) add complexity</li>
<li>Tasks the needs Tashkeel like Speech synthesis Grammar checking (need tashkeel) like arabic poetry tasks ..etc</li>
</ul>
</section>
<section id="the-evaluation-framework" class="level2">
<h2 class="anchored" data-anchor-id="the-evaluation-framework">The Evaluation Framework</h2>
<p>I built a simple evaluation function to measure tokenizer quality:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> evaluate_tokenizer(text, tokenizer):</span>
<span id="cb1-2">    number_of_tokens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(tokenizer.tokenize(text))</span>
<span id="cb1-3">    number_of_bytes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(text.encode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'utf-8'</span>))</span>
<span id="cb1-4">    number_of_words <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(text.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" "</span>))</span>
<span id="cb1-5">    fertility <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> number_of_tokens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> number_of_words </span>
<span id="cb1-6">    compression_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> number_of_bytes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> number_of_tokens </span>
<span id="cb1-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {</span>
<span id="cb1-8">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"fertility"</span>: fertility,</span>
<span id="cb1-9">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"compression_ratio"</span>: compression_ratio,</span>
<span id="cb1-10">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"total_tokens"</span>: number_of_tokens</span>
<span id="cb1-11">    }</span></code></pre></div></div>
<section id="key-metrics" class="level3">
<h3 class="anchored" data-anchor-id="key-metrics">Key Metrics</h3>
<ol type="1">
<li><strong>Fertility Rate</strong> (tokens/word): Lower is better. Measures how many tokens are needed per word.</li>
<li><strong>Compression Ratio</strong> (bytes/token): Higher is better. Measures how efficiently the tokenizer compresses text.</li>
<li><strong>Total Tokens</strong>: The raw count for the given text.</li>
</ol>
</section>
</section>
<section id="the-contenders" class="level2">
<h2 class="anchored" data-anchor-id="the-contenders">The Contenders</h2>
<section id="aramodernbert" class="level3">
<h3 class="anchored" data-anchor-id="aramodernbert">AraModernBert</h3>
<ul>
<li><strong>Vocabulary</strong>: 50,280 tokens</li>
<li><strong>Training Data</strong>: 100GB of Arabic text</li>
<li><strong>Architecture</strong>: ModernBERT with transtokenization</li>
<li><strong>Context Window</strong>: 8,192 tokens</li>
</ul>
</section>
<section id="arabert-v2" class="level3">
<h3 class="anchored" data-anchor-id="arabert-v2">AraBERT v2</h3>
<ul>
<li><strong>Vocabulary</strong>: ~30,000 tokens</li>
<li><strong>Training Data</strong>: 77GB of Arabic text</li>
<li><strong>Architecture</strong>: BERT-base</li>
<li><strong>Pre-segmentation</strong>: Uses Farasa segmenter</li>
</ul>
</section>
</section>
<section id="test-results" class="level2">
<h2 class="anchored" data-anchor-id="test-results">Test Results</h2>
<p>I tested both tokenizers on three different Arabic texts with Encoder only for now more to come later!:</p>
<section id="test-1-modern-standard-arabic" class="level3">
<h3 class="anchored" data-anchor-id="test-1-modern-standard-arabic">Test 1: Modern Standard Arabic</h3>
<p><strong>Text</strong>: ‚ÄúŸÖÿ±ÿ≠ÿ®ÿß ŸÉŸäŸÅ ÿ≠ÿßŸÑŸÉ ÿßŸÑŸäŸàŸÖ‚Äù</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Tokenizer</th>
<th>Fertility</th>
<th>Compression Ratio</th>
<th>Total Tokens</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AraModernBert</td>
<td>1.25</td>
<td>7.4</td>
<td>5</td>
</tr>
<tr class="even">
<td>AraBERT v2</td>
<td>1.5</td>
<td>6.17</td>
<td>6</td>
</tr>
</tbody>
</table>
</section>
<section id="test-2-egyptian-dialect" class="level3">
<h3 class="anchored" data-anchor-id="test-2-egyptian-dialect">Test 2: Egyptian Dialect</h3>
<p><strong>Text</strong>: ‚Äúÿ•ÿ≤ŸäŸÉ Ÿäÿß ÿµÿßÿ≠ÿ®Ÿä ÿπÿßŸÖŸÑ ÿ•ŸäŸá‚Äù</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Tokenizer</th>
<th>Fertility</th>
<th>Compression Ratio</th>
<th>Total Tokens</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AraModernBert</td>
<td>1.2</td>
<td>6.67</td>
<td>6</td>
</tr>
<tr class="even">
<td>AraBERT v2</td>
<td>1.4</td>
<td>5.71</td>
<td>7</td>
</tr>
</tbody>
</table>
</section>
<section id="test-3-technicalformal-text" class="level3">
<h3 class="anchored" data-anchor-id="test-3-technicalformal-text">Test 3: Technical/Formal Text</h3>
<p><strong>Text</strong>: ‚ÄúÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä Ÿäÿ∫Ÿäÿ± ÿßŸÑÿπÿßŸÑŸÖ ÿ®ÿ≥ÿ±ÿπÿ© ŸÉÿ®Ÿäÿ±ÿ©‚Äù</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Tokenizer</th>
<th>Fertility</th>
<th>Compression Ratio</th>
<th>Total Tokens</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AraModernBert</td>
<td>1.17</td>
<td>10.71</td>
<td>7</td>
</tr>
<tr class="even">
<td>AraBERT v2</td>
<td>2.0</td>
<td>6.25</td>
<td><strong>12</strong></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="key-findings" class="level2">
<h2 class="anchored" data-anchor-id="key-findings">Key Findings</h2>
<section id="aramodernbert-is-consistently-more-efficient" class="level3">
<h3 class="anchored" data-anchor-id="aramodernbert-is-consistently-more-efficient">1. AraModernBert is Consistently More Efficient</h3>
<p>Across all three tests, AraModernBert showed:</p>
<ul>
<li><p><strong>Lower fertility</strong> (15-42% fewer tokens per word)</p></li>
<li><p><strong>Higher compression ratio</strong> (14-71% better compression)</p></li>
<li><p><strong>Fewer total tokens</strong> needed for the same text</p></li>
</ul>
</section>
<section id="the-gap-widens-with-complex-text" class="level3">
<h3 class="anchored" data-anchor-id="the-gap-widens-with-complex-text">2. The Gap Widens with Complex Text</h3>
<p>The most dramatic difference appeared in Test 3 (technical text):</p>
<ul>
<li><p>AraModernBert: 7 tokens</p></li>
<li><p>AraBERT v2: 12 tokens (<strong>71% more!</strong>)</p></li>
</ul>
<p>This means for an 8K context window, AraModernBert can fit significantly more Arabic text.</p>
</section>
<section id="both-handle-dialects-reasonably-well" class="level3">
<h3 class="anchored" data-anchor-id="both-handle-dialects-reasonably-well">3. Both Handle Dialects Reasonably Well</h3>
<p>The Egyptian dialect test (Test 2) showed both tokenizers maintained similar efficiency to MSA, though AraModernBert still outperformed.</p>
</section>
</section>
<section id="why-aramodernbert-performs-better" class="level2">
<h2 class="anchored" data-anchor-id="why-aramodernbert-performs-better">Why AraModernBert Performs Better</h2>
<section id="larger-vocabulary-50k-vs-30k-tokens" class="level3">
<h3 class="anchored" data-anchor-id="larger-vocabulary-50k-vs-30k-tokens">Larger Vocabulary (50K vs 30K tokens)</h3>
<p>More tokens means the model can learn longer, more common Arabic word chunks as single tokens.</p>
<p>This is especially important for Arabic‚Äôs morphologically rich structure.</p>
</section>
<section id="more-training-data-100gb-vs-77gb" class="level3">
<h3 class="anchored" data-anchor-id="more-training-data-100gb-vs-77gb">More Training Data (100GB vs 77GB)</h3>
<p>More data leads to better byte-pair encoding merges that reflect actual Arabic usage patterns.</p>
</section>
<section id="modern-architecture" class="level3">
<h3 class="anchored" data-anchor-id="modern-architecture">Modern Architecture</h3>
<p>AraModernBert uses transtokenization - a technique that optimally initializes embeddings when creating the tokenizer, leading to better learned representations.</p>
</section>
<section id="recency-advantage" class="level3">
<h3 class="anchored" data-anchor-id="recency-advantage">Recency Advantage</h3>
<p>Trained in 2024 vs 2020, AraModernBert benefits from more recent data and improved training techniques.</p>
</section>
</section>
<section id="practical-implications" class="level2">
<h2 class="anchored" data-anchor-id="practical-implications">Practical Implications</h2>
<section id="for-model-training" class="level3">
<h3 class="anchored" data-anchor-id="for-model-training">For Model Training</h3>
<ul>
<li><strong>Context efficiency</strong>: AraModernBert lets you fit ~40% more Arabic text in the same context window</li>
<li><strong>Cost savings</strong>: Fewer tokens = lower training costs</li>
<li><strong>Better performance</strong>: More efficient tokenization often correlates with better downstream task performance</li>
</ul>
</section>
<section id="for-production-systems" class="level3">
<h3 class="anchored" data-anchor-id="for-production-systems">For Production Systems</h3>
<ul>
<li><strong>API costs</strong>: Pay per token, so more efficient tokenization = lower costs</li>
<li><strong>Latency</strong>: Fewer tokens to process = faster inference</li>
<li><strong>Memory</strong>: Smaller token sequences = lower memory footprint</li>
</ul>
</section>
</section>
<section id="should-you-train-your-own-tokenizer" class="level2">
<h2 class="anchored" data-anchor-id="should-you-train-your-own-tokenizer">Should You Train Your Own Tokenizer?</h2>
<p>After this evaluation, here‚Äôs my thinking:</p>
<p><strong>Use AraModernBert if:</strong></p>
<ul>
<li>You‚Äôre working with Modern Standard Arabic or mixed dialects</li>
<li>You want state-of-the-art efficiency out of the box</li>
<li>You don‚Äôt have massive compute resources for training</li>
</ul>
<p><strong>Train your own if:</strong></p>
<ul>
<li>You have a very specific domain (medical, legal, etc.)</li>
<li>You‚Äôre working with a specific dialect extensively (pure Egyptian, Levantine, etc.)</li>
<li>You have unique requirements (handling tashkeel differently, etc.)</li>
</ul>
<p>For my Egyptian Arabic use case, I‚Äôm leaning toward using AraModernBert‚Äôs tokenizer as-is, since:</p>
<ol type="1">
<li>It already handles Egyptian dialect reasonably well</li>
<li>The 50K vocabulary is large enough to be flexible</li>
<li>Training a custom tokenizer requires significant effort and data</li>
</ol>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next Steps</h2>
<ol type="1">
<li><strong>Test on real data</strong>: Evaluate on actual Egyptian Arabic corpus (FineWeb Egyptian)</li>
<li><strong>Compare with SentencePiece</strong>: Test a SentencePiece tokenizer trained on Arabic</li>
<li><strong>Measure downstream performance</strong>: Tokenizer efficiency doesn‚Äôt always equal better model performance</li>
<li><strong>Investigate tashkeel handling</strong>: How do these tokenizers handle diacritical marks?</li>
</ol>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Tokenizer evaluation revealed that <strong>AraModernBert significantly outperforms AraBERT v2</strong> in efficiency metrics, with 15-71% fewer tokens needed for the same Arabic text.</p>
<p>This translates to real cost savings and performance improvements in production systems.</p>
<p>The key lesson: <strong>don‚Äôt assume all tokenizers are equal</strong>. A few hours of evaluation can save months of headaches and significant costs down the line.</p>
</section>
<section id="code-repository" class="level2">
<h2 class="anchored" data-anchor-id="code-repository">Code Repository</h2>
<p>The complete evaluation code is available as a simple function:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> evaluate_tokenizer(text, tokenizer):</span>
<span id="cb2-2">    number_of_tokens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(tokenizer.tokenize(text))</span>
<span id="cb2-3">    number_of_bytes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(text.encode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'utf-8'</span>))</span>
<span id="cb2-4">    number_of_words <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(text.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" "</span>))</span>
<span id="cb2-5">    fertility <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> number_of_tokens <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> number_of_words </span>
<span id="cb2-6">    compression_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> number_of_bytes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> number_of_tokens </span>
<span id="cb2-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {</span>
<span id="cb2-8">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"fertility"</span>: fertility,</span>
<span id="cb2-9">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"compression_ratio"</span>: compression_ratio,</span>
<span id="cb2-10">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"total_tokens"</span>: number_of_tokens</span>
<span id="cb2-11">    }</span></code></pre></div></div>
<p>this post was built with guide from Amazing <a href="https://solve.it.com/?via_id=t1hpiqbn">Solveit</a></p>


</section>

 ]]></description>
  <category>blogging</category>
  <category>til</category>
  <category>nlp</category>
  <category>ai</category>
  <guid>https://kareemai.com/til/tils/2025-10-21.html</guid>
  <pubDate>Mon, 20 Oct 2025 21:00:00 GMT</pubDate>
  <media:content url="https://kareemai.com/til/tils/til.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Connecting with ideas in ML</title>
  <dc:creator>kareem </dc:creator>
  <link>https://kareemai.com/til/tils/2025-06-06-til.html</link>
  <description><![CDATA[ 





<p>Here is the corrected version with grammar errors fixed and improved clarity, while preserving the original meaning and tone:</p>
<ul>
<li>Get paid for your knowledge!</li>
<li>I‚Äôm not interested in all this talk about being replaced by AI. If AI can replace an engineer‚Äôs mind, it will replace everything in the world.</li>
<li>When this happens, it won‚Äôt really matter!</li>
<li>I got my first job one year after graduation with a good salary for my experience and the local currency.</li>
<li>ÿßŸÑÿ≠ŸÖÿØ ŸàÿßŸÑÿ¥ŸÉÿ± ŸàÿßŸÑŸÅÿ∂ŸÑ ŸÑŸÑŸá Ÿàÿ≠ÿØŸá.</li>
<li>Actually, I got two jobs, not one!</li>
<li>I landed a full-time AI Engineer position at a startup that is already operational with its own customers, plus a part-time role with flexible meetings. I‚Äôm the wildcard‚Äîmaybe even the AI‚Äîin this startup, which is set to receive funding in the coming weeks.</li>
<li>This might give the impression that I‚Äôm a great engineer, but I‚Äôm not!</li>
<li>I‚Äôm not good at programming; I‚Äôm below average and can barely do cool stuff on my own. I only know some solutions to try, and I‚Äôm familiar with multiple resources and places to get help. I love asking more experienced people for feedback.</li>
<li>I can only say that I love this field a lot! It‚Äôs very interesting. With just a laptop and my mind, I can create things the world needs and will pay for!</li>
<li>This may sound silly, but it‚Äôs an amazing idea to think about, especially for someone like me who doesn‚Äôt enjoy the outside world or dealing with people in real life.</li>
<li>What I like is that the world is connected, and you can gain recognition quickly if you‚Äôre doing real work and building connections with people in your field.</li>
<li>I‚Äôm interested in Machine Learning and focus on niche areas where not many people are working. In my language, there‚Äôs little competition, which gives me a unique edge! But in reality, I‚Äôm below average; the basics I explore in these areas are enough for now.</li>
<li>I don‚Äôt advise anyone to do this, but I want to say that when you try your best, reflect on your goals, start crafting your ideas, and engage with others‚Äô ideas, your influence and impact will grow significantly.</li>
<li>I‚Äôve met many ordinary people who simply read documentation, ask questions about what they don‚Äôt know, and are beginners, yet others look at them and think, ‚ÄúWow, they must be geniuses!‚Äù</li>
<li>But they‚Äôre not! You may have more knowledge and experience than them, but business, your birthplace, and the college you graduated from are strong factors in determining your path.</li>
<li>You can reach great places without these, but they accelerate your progress!</li>
<li>We all know the story of the child who learned programming at 11 years old and now, at 25, codes as easily as walking.</li>
<li>What I‚Äôve found is that you can still build great connections with someone who has 10 years more experience than you, and they might even say, ‚ÄúYou‚Äôre a smart person! I want you to work with me.‚Äù</li>
<li>We‚Äôre all limited, and there will always be gaps that require other minds to fill. The great thing is that we grow faster when we connect with such minds!</li>
<li>I really admire the work from the AnswerDotAI team. I find that Jeremy Howard has a profound impact on how I think about AI and learning.</li>
<li>The courses I‚Äôve taken with him, the community, and the tools make me feel ahead of the curve and capable of creating things!</li>
<li>When you start following people like Omar Khatab, Benjamin, Antoine, Tom Arson, and many others, and interact with their work, you begin to gain weight in these spaces. I‚Äôm not just talking about getting a job‚ÄîI‚Äôm talking about the level of ideas.</li>
<li>There are people watching, people building tools around concepts, and people creating new concepts!</li>
<li>I wish I could reach the cutting edge of knowledge. This may not be precise, but I see it as a journey I want to pursue.</li>
<li>There‚Äôs more to say, but this world has immense potential for smart people‚Äînot in terms of marketing or being a shallow influencer.</li>
<li>I want to say that I have many ideas I want to create and share with other minds. I believe I‚Äôll be able to make a positive impact in the areas I‚Äôm passionate about and improve Islamic tech and Arabic NLP.</li>
</ul>



 ]]></description>
  <category>blogging</category>
  <category>til</category>
  <guid>https://kareemai.com/til/tils/2025-06-06-til.html</guid>
  <pubDate>Thu, 05 Jun 2025 21:00:00 GMT</pubDate>
  <media:content url="https://kareemai.com/til/tils/til.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Pylate Day 1</title>
  <dc:creator>kareem </dc:creator>
  <link>https://kareemai.com/til/tils/2025-05-28-til.html</link>
  <description><![CDATA[ 





<section id="pylate-colbert-evaluation---complete-learning-guide" class="level1">
<h1>PyLate &amp; ColBERT Evaluation - Complete Learning Guide</h1>
<section id="core-concepts" class="level2">
<h2 class="anchored" data-anchor-id="core-concepts">Core Concepts</h2>
<section id="what-is-pylate" class="level3">
<h3 class="anchored" data-anchor-id="what-is-pylate">What is PyLate?</h3>
<p><strong>Q: What is PyLate and what does it do?</strong> A: PyLate is a Python library for vector retrieval and search, specifically designed for ColBERT models. It provides tools for indexing documents, encoding queries, and performing similarity search.</p>
</section>
<section id="what-is-colbert" class="level3">
<h3 class="anchored" data-anchor-id="what-is-colbert">What is ColBERT?</h3>
<p><strong>Q: How does ColBERT work?</strong> A: ColBERT (Contextualized Late Interaction over BERT) creates dense vector representations for documents and queries, then uses late interaction (token-level matching) for retrieval instead of single vector similarity.</p>
</section>
</section>
<section id="dataset-formats" class="level2">
<h2 class="anchored" data-anchor-id="dataset-formats">Dataset Formats</h2>
<section id="format-1-tripletmulti-negative-format" class="level3">
<h3 class="anchored" data-anchor-id="format-1-tripletmulti-negative-format">Format 1: Triplet/Multi-negative Format</h3>
<p><strong>Q: What does triplet format look like?</strong> A: Each row contains: - <code>query</code>: The search query - <code>positive</code>: One relevant document<br>
- <code>negative1</code>, <code>negative2</code>, ‚Ä¶: Multiple irrelevant documents</p>
<p><strong>Pros</strong>: Good for training with hard negatives <strong>Cons</strong>: No separate corpus, harder to evaluate</p>
</section>
<section id="format-2-structured-retrieval-format" class="level3">
<h3 class="anchored" data-anchor-id="format-2-structured-retrieval-format">Format 2: Structured Retrieval Format</h3>
<p><strong>Q: What does structured format contain?</strong> A: Three separate components: - <code>corpus</code>: All documents with IDs - <code>queries</code>: All queries with IDs - <code>qrels</code>: Relevance judgments (query-doc pairs)</p>
<p><strong>Pros</strong>: Standard IR evaluation format, works with PyLate directly <strong>Cons</strong>: More complex structure</p>
</section>
<section id="format-3-passage-ranking-format" class="level3">
<h3 class="anchored" data-anchor-id="format-3-passage-ranking-format">Format 3: Passage Ranking Format</h3>
<p><strong>Q: How does passage ranking format work?</strong> A: Each row has: - <code>query_id</code>, <code>query</code>: Query information - <code>positive_passages</code>: List of relevant documents - <code>negative_passages</code>: List of irrelevant documents</p>
<p><strong>Pros</strong>: Multiple positives/negatives per query, rich annotations <strong>Cons</strong>: Requires extraction to create corpus</p>
</section>
</section>
<section id="data-conversion-strategies" class="level2">
<h2 class="anchored" data-anchor-id="data-conversion-strategies">Data Conversion Strategies</h2>
<section id="memory-efficient-file-writing" class="level3">
<h3 class="anchored" data-anchor-id="memory-efficient-file-writing">Memory-Efficient File Writing</h3>
<p><strong>Q: How do you handle large datasets efficiently?</strong> A: Stream processing with direct file writing:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'corpus.jsonl'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'w'</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> f:</span>
<span id="cb1-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> el <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> dataset[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'corpus'</span>]:</span>
<span id="cb1-3">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> el[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'corpus-id'</span>] <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> el[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'text'</span>]:</span>
<span id="cb1-4">            json.dump({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"_id"</span>: el[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'corpus-id'</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text"</span>: el[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'text'</span>]}, f)</span>
<span id="cb1-5">            f.write(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div></div>
<p><strong>Pros</strong>: Low memory usage, handles any dataset size <strong>Cons</strong>: Requires file I/O, slightly slower</p>
</section>
<section id="pylate-file-requirements" class="level3">
<h3 class="anchored" data-anchor-id="pylate-file-requirements">PyLate File Requirements</h3>
<p><strong>Q: What files does BEIR datasets is ?</strong> A: - <code>corpus.jsonl</code>: <code>{"_id": "doc1", "text": "document text"}</code> - <code>queries.jsonl</code>: <code>{"_id": "q1", "text": "query text"}</code> - <code>qrels/split.tsv</code>: <code>query_id\tdoc_id\tscore</code></p>
<p><strong>Critical</strong>: File names and folder structure must match exactly</p>
</section>
<section id="direct-dictionary-conversion" class="level3">
<h3 class="anchored" data-anchor-id="direct-dictionary-conversion">Direct Dictionary Conversion</h3>
<p><strong>Q: How do you convert without files?</strong> A: Transform to PyLate‚Äôs expected return format:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">documents <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"id"</span>: doc_id, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text"</span>: text} <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> doc_id, text <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> corpus.items()]</span>
<span id="cb2-2">queries <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(queries.values())</span>
<span id="cb2-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># qrels stays as dictionary</span></span></code></pre></div></div>
<p><strong>Pros</strong>: Faster, no file operations <strong>Cons</strong>: Must match exact format, harder to debug</p>
</section>
</section>
<section id="evaluation-approaches" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-approaches">Evaluation Approaches</h2>
<section id="custom-evaluation-with-ranx" class="level3">
<h3 class="anchored" data-anchor-id="custom-evaluation-with-ranx">Custom Evaluation with ranx</h3>
<p><strong>Q: When do you use ranx for evaluation?</strong> A: When you have non-standard formats or want custom metrics:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">qrels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Qrels(qrels_dict)</span>
<span id="cb3-2">run <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Run(run_dict)</span>
<span id="cb3-3">metrics <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> evaluate(qrels, run, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ndcg@5"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"map@5"</span>])</span></code></pre></div></div>
<p><strong>Pros</strong>: Flexible, works with any format <strong>Cons</strong>: Manual setup required</p>
</section>
<section id="pylate-built-in-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="pylate-built-in-evaluation">PyLate Built-in Evaluation</h3>
<p><strong>Q: When do you use PyLate‚Äôs evaluation?</strong> A: When data is in standard BEIR format with proper file structure</p>
<p><strong>Pros</strong>: Standardized, less code <strong>Cons</strong>: Strict format requirements</p>
</section>
</section>
<section id="indexing-strategies" class="level2">
<h2 class="anchored" data-anchor-id="indexing-strategies">Indexing Strategies</h2>
<section id="plaid-index" class="level3">
<h3 class="anchored" data-anchor-id="plaid-index">PLAID Index</h3>
<p><strong>Q: What is PLAID indexing?</strong> A: PyLate‚Äôs efficient indexing method for ColBERT embeddings, supporting fast similarity search</p>
<p><strong>Key Parameters</strong>: - <code>index_folder</code>: Where to store index - <code>index_name</code>: Identifier for the index<br>
- <code>override=True</code>: Overwrites existing index</p>
</section>
<section id="document-processing" class="level3">
<h3 class="anchored" data-anchor-id="document-processing">Document Processing</h3>
<p><strong>Q: How do you prepare documents for indexing?</strong> A: 1. Extract unique documents from all sources 2. Create document IDs and embeddings 3. Add to index with <code>add_documents()</code></p>
<p><strong>Important</strong>: Use <code>is_query=False</code> for documents, <code>is_query=True</code> for queries</p>
</section>
</section>
<section id="common-pitfalls-solutions" class="level2">
<h2 class="anchored" data-anchor-id="common-pitfalls-solutions">Common Pitfalls &amp; Solutions</h2>
<section id="file-format-issues" class="level3">
<h3 class="anchored" data-anchor-id="file-format-issues">File Format Issues</h3>
<p><strong>Q: What are common file format mistakes?</strong> A: - Wrong file extensions (.csv instead of .tsv) - Incorrect folder structure (missing qrels folder) - Wrong field names (<code>id</code> vs <code>_id</code>)</p>
</section>
<section id="data-extraction-problems" class="level3">
<h3 class="anchored" data-anchor-id="data-extraction-problems">Data Extraction Problems</h3>
<p><strong>Q: How do you handle variable-length lists?</strong> A: Use nested loops for negative passages:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> row <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> dataset:</span>
<span id="cb4-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> neg_doc <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> row[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'negative_passages'</span>]:</span>
<span id="cb4-3">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># process each negative document</span></span></code></pre></div></div>
</section>
<section id="memory-management" class="level3">
<h3 class="anchored" data-anchor-id="memory-management">Memory Management</h3>
<p><strong>Q: How do you avoid memory issues?</strong> A: - Process datasets in chunks - Use generators instead of lists - Write to files incrementally - Use dictionaries to avoid duplicates</p>
</section>
</section>
<section id="performance-considerations" class="level2">
<h2 class="anchored" data-anchor-id="performance-considerations">Performance Considerations</h2>
<section id="batch-processing" class="level3">
<h3 class="anchored" data-anchor-id="batch-processing">Batch Processing</h3>
<p><strong>Q: How do you optimize encoding speed?</strong> A: Use appropriate batch sizes: - Larger batches: Faster but more memory - Smaller batches: Slower but memory-safe - Typical: <code>batch_size=32</code> or <code>batch_size=64</code></p>
</section>
<section id="index-management" class="level3">
<h3 class="anchored" data-anchor-id="index-management">Index Management</h3>
<p><strong>Q: How do you manage multiple indexes?</strong> A: Use descriptive names and separate folders: - <code>index_folder="arabic_index"</code> - <code>index_name="gte-multilingual-base"</code></p>
</section>
</section>
<section id="evaluation-metrics" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-metrics">Evaluation Metrics</h2>
<section id="standard-ir-metrics" class="level3">
<h3 class="anchored" data-anchor-id="standard-ir-metrics">Standard IR Metrics</h3>
<p><strong>Q: What metrics should you track?</strong> A: - <strong>NDCG@k</strong>: Normalized discounted cumulative gain - <strong>MAP@k</strong>: Mean average precision<br>
- <strong>Recall@k</strong>: Proportion of relevant docs retrieved - <strong>Precision@k</strong>: Proportion of retrieved docs that are relevant</p>
</section>
<section id="interpreting-results" class="level3">
<h3 class="anchored" data-anchor-id="interpreting-results">Interpreting Results</h3>
<p><strong>Q: What makes good retrieval performance?</strong> A: - NDCG@5 &gt; 0.7: Excellent - NDCG@5 &gt; 0.5: Good<br>
- NDCG@5 &gt; 0.3: Acceptable - NDCG@5 &lt; 0.3: Needs improvement</p>
<p>This comprehensive guide covers all the key concepts, trade-offs, and practical considerations for working with PyLate and ColBERT evaluation pipelines.</p>


</section>
</section>
</section>

 ]]></description>
  <category>blogging</category>
  <category>til</category>
  <guid>https://kareemai.com/til/tils/2025-05-28-til.html</guid>
  <pubDate>Tue, 27 May 2025 21:00:00 GMT</pubDate>
  <media:content url="https://kareemai.com/til/tils/til.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>RAG RAPTOR: Complete Guide to Meta‚Äôs RAPTOR RAG, DFloat11 Compression &amp; Pyrefly Type Checker</title>
  <dc:creator>kareem </dc:creator>
  <link>https://kareemai.com/til/tils/2025-05-25-til.html</link>
  <description><![CDATA[ 





<section id="rag-raptor-dfloat11-and-pyrefly-metas-latest-open-source-tools" class="level2">
<h2 class="anchored" data-anchor-id="rag-raptor-dfloat11-and-pyrefly-metas-latest-open-source-tools">RAG RAPTOR, DFloat11 and Pyrefly: Meta‚Äôs Latest Open Source Tools</h2>
<p>Over the last 2 days, I‚Äôve been exploring some fascinating new open source tools from Meta and other organizations. These tools are revolutionizing how we approach RAG (Retrieval-Augmented Generation), model compression, and Python development. Let me break down what I‚Äôve learned:</p>
<ol type="1">
<li><strong>RAPTOR RAG</strong> - Meta‚Äôs tree-based retrieval technique</li>
<li><strong>DFloat11</strong> - Lossless compression for LLMs<br>
</li>
<li><strong>Pyrefly</strong> - Fast Python type checker in Rust</li>
<li><strong>FastEmbed</strong> - Efficient embedding generation</li>
</ol>
</section>
<section id="what-is-raptor-rag-complete-guide-to-metas-raptor-technique" class="level2">
<h2 class="anchored" data-anchor-id="what-is-raptor-rag-complete-guide-to-metas-raptor-technique">What is RAPTOR RAG? Complete Guide to Meta‚Äôs RAPTOR Technique</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://kareemai.com/til/tils/images/raptor.png" class="img-fluid figure-img"></p>
<figcaption>raptor rag</figcaption>
</figure>
</div>
<p>RAPTOR RAG is Meta‚Äôs innovative technique designed to improve document retrieval performance. While they claim a 20% improvement, my testing shows mixed results depending on the use case.</p>
<section id="understanding-raptor-rag" class="level3">
<h3 class="anchored" data-anchor-id="understanding-raptor-rag">Understanding RAPTOR RAG</h3>
<p><strong>RAPTOR</strong> stands for ‚ÄúRecursive Abstractive Processing for Tree-Organized Retrieval.‚Äù It‚Äôs a tree-based approach that constructs hierarchical representations of your documents by recursively clustering text chunks based on their vector embeddings and generating summaries of those clusters.</p>
<p>The core innovation is building a tree structure from the bottom up, which helps solve the fundamental limitation of traditional RAG systems: retrieving only a few short, contiguous text chunks that limit their ability to represent large-scale discourse structure.</p>
</section>
<section id="the-problem-raptor-rag-solves" class="level3">
<h3 class="anchored" data-anchor-id="the-problem-raptor-rag-solves">The Problem RAPTOR RAG Solves</h3>
<p>Traditional RAG systems struggle with thematic questions that require integrating knowledge from multiple parts of a document. This is particularly relevant for complex queries like understanding an entire book or analyzing long-form content.</p>
<p><strong>Example:</strong> Consider the fairy tale of Cinderella and the question ‚ÄúHow did Cinderella reach her happy ending?‚Äù Traditional RAG‚Äôs top-k retrieved short contiguous texts won‚Äôt contain enough context to answer this comprehensively.</p>
</section>
<section id="how-raptor-rag-works" class="level3">
<h3 class="anchored" data-anchor-id="how-raptor-rag-works">How RAPTOR RAG Works</h3>
<p>RAPTOR solves this by using a tree structure to capture both high-level and low-level details about text. The process involves:</p>
<ol type="1">
<li><strong>Clustering text chunks</strong> based on semantic similarity</li>
<li><strong>Generating summaries</strong> for each cluster using language models</li>
<li><strong>Repeating the process</strong> recursively to build a hierarchical tree</li>
<li><strong>Tree-based retrieval</strong> during query time</li>
</ol>
</section>
<section id="model-based-summarization-in-raptor" class="level3">
<h3 class="anchored" data-anchor-id="model-based-summarization-in-raptor">Model-Based Summarization in RAPTOR</h3>
<p>After clustering nodes using Gaussian Mixture Models, each cluster is sent to a language model (typically GPT-3.5-turbo) for summarization. This step transforms large chunks of text into concise, coherent summaries, condensing potentially large volumes of retrieved information into manageable sizes.</p>
</section>
<section id="querying-raptor-rag" class="level3">
<h3 class="anchored" data-anchor-id="querying-raptor-rag">Querying RAPTOR RAG</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://kareemai.com/til/tils/images/raptor_query.png" class="img-fluid figure-img"></p>
<figcaption>query RAPTOR rag</figcaption>
</figure>
</div>
<p>RAPTOR supports two main querying approaches:</p>
<ol type="1">
<li><strong>Tree Traversal</strong> - Navigate through the hierarchical structure</li>
<li><strong>Collapsed Tree Retrieval</strong> - Flatten the tree for faster retrieval</li>
</ol>
</section>
<section id="raptor-rag-performance-analysis" class="level3">
<h3 class="anchored" data-anchor-id="raptor-rag-performance-analysis">RAPTOR RAG Performance Analysis</h3>
<p>While the 20% improvement claim is appealing, my testing reveals some limitations:</p>
<ul>
<li><strong>Accuracy gains are marginal</strong> compared to SBERT-based solutions</li>
<li><strong>Computational overhead</strong> is significant for creation and retrieval</li>
<li><strong>Best use case</strong> is document summarization rather than question answering</li>
<li><strong>Information flow</strong> can be repetitive in some scenarios</li>
</ul>
</section>
<section id="raptor-rag-for-document-summarization" class="level3">
<h3 class="anchored" data-anchor-id="raptor-rag-for-document-summarization">RAPTOR RAG for Document Summarization</h3>
<p>The most compelling use case for RAPTOR is document summarization. By having detailed, medium, and high-level information representations, you can create better summaries by:</p>
<ul>
<li><strong>Dividing the summarization task</strong> across multiple levels</li>
<li><strong>Pre-computing summaries</strong> at different granularities</li>
<li><strong>Reducing the burden</strong> on the final LLM for summary generation</li>
</ul>
</section>
</section>
<section id="dfloat11-efficient-lossless-compression-for-llms" class="level2">
<h2 class="anchored" data-anchor-id="dfloat11-efficient-lossless-compression-for-llms">DFloat11: Efficient, Lossless Compression for LLMs</h2>
<p>DFloat11 (DF11) is a novel, mathematically lossless compression technique that reduces large language model memory usage by about 30% with zero accuracy loss. Unlike traditional quantization methods that can degrade model quality, DF11 uses Huffman coding to compress only the predictable exponent bits of model weights.</p>
<section id="how-dfloat11-works" class="level3">
<h3 class="anchored" data-anchor-id="how-dfloat11-works">How DFloat11 Works</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://kareemai.com/til/tils/images/df11.png" class="img-fluid figure-img"></p>
<figcaption>DFloat11 encoding</figcaption>
</figure>
</div>
<p>DFloat11‚Äôs compression strategy is elegant in its simplicity:</p>
<ul>
<li><strong>Sign and Fraction Bits:</strong> Kept unchanged as they contain high-entropy information</li>
<li><strong>Exponent Bits:</strong> Compressed using a precomputed Huffman tree, replacing the fixed 8-bit exponent with variable-length codes</li>
<li><strong>Average Savings:</strong> About 5 bits per weight on average</li>
</ul>
</section>
<section id="dfloat11-storage-and-decoding" class="level3">
<h3 class="anchored" data-anchor-id="dfloat11-storage-and-decoding">DFloat11 Storage and Decoding</h3>
<p><strong>Storage Architecture:</strong> - Sign/fraction and exponent bits stored separately - Small header containing the Huffman codebook - Efficient packing for minimal overhead</p>
<p><strong>Runtime Decoding:</strong> - Original weights quickly reconstructed by combining sign/fraction block with decoded exponent - Enables fast, parallel processing on GPUs - No performance penalty during inference</p>
</section>
<section id="dfloat11-key-benefits" class="level3">
<h3 class="anchored" data-anchor-id="dfloat11-key-benefits">DFloat11 Key Benefits</h3>
<ul>
<li><strong>30% reduction</strong> in model size compared to bf16</li>
<li><strong>100% identical accuracy</strong> to the original model</li>
<li><strong>Universal applicability</strong> to any transformer-based LLM</li>
<li><strong>Zero training required</strong> - works with existing models</li>
</ul>
</section>
</section>
<section id="pyrefly-fast-python-type-checker-in-rust" class="level2">
<h2 class="anchored" data-anchor-id="pyrefly-fast-python-type-checker-in-rust">Pyrefly: Fast Python Type Checker in Rust</h2>
<p>Pyrefly is a blazingly fast Python type checker written in Rust, designed to provide near-instantaneous type checking for large Python codebases.</p>
<section id="installing-pyrefly-with-vs-code" class="level3">
<h3 class="anchored" data-anchor-id="installing-pyrefly-with-vs-code">Installing Pyrefly with VS Code</h3>
<p>I installed Pyrefly into VS Code and tested it with several projects including LlamaIndex and TypeCodebase. The experience was impressive:</p>
<ul>
<li><strong>Lightning-fast type checking</strong> compared to traditional tools</li>
<li><strong>Seamless VS Code integration</strong> with real-time feedback</li>
<li><strong>Excellent performance</strong> on large codebases</li>
<li><strong>Rust-powered reliability</strong> with minimal memory usage</li>
</ul>
</section>
<section id="pyrefly-vs-traditional-type-checkers" class="level3">
<h3 class="anchored" data-anchor-id="pyrefly-vs-traditional-type-checkers">Pyrefly vs Traditional Type Checkers</h3>
<p>Pyrefly‚Äôs Rust implementation provides significant advantages:</p>
<ul>
<li><strong>Speed improvements</strong> of 10-100x over Python-based type checkers</li>
<li><strong>Lower memory footprint</strong> for large projects</li>
<li><strong>Better error reporting</strong> with precise location information</li>
<li><strong>Incremental checking</strong> for faster subsequent runs</li>
</ul>
</section>
</section>
<section id="llamaindex-raptor-integration" class="level2">
<h2 class="anchored" data-anchor-id="llamaindex-raptor-integration">LlamaIndex RAPTOR Integration</h2>
<p>For those using LlamaIndex, RAPTOR RAG can be integrated to improve document retrieval performance. The tree-based approach works particularly well with LlamaIndex‚Äôs document processing pipeline.</p>
<section id="implementation-considerations" class="level3">
<h3 class="anchored" data-anchor-id="implementation-considerations">Implementation Considerations</h3>
<p>When implementing RAPTOR with LlamaIndex:</p>
<ul>
<li><strong>Consider the computational cost</strong> of building the tree structure</li>
<li><strong>Evaluate performance gains</strong> for your specific use case</li>
<li><strong>Test with your document types</strong> before full deployment</li>
<li><strong>Monitor memory usage</strong> during tree construction</li>
</ul>
</section>
</section>
<section id="meta-pyrefly-and-open-source-ecosystem" class="level2">
<h2 class="anchored" data-anchor-id="meta-pyrefly-and-open-source-ecosystem">Meta Pyrefly and Open Source Ecosystem</h2>
<p>Meta‚Äôs contribution to the Python development ecosystem through Pyrefly demonstrates their commitment to developer productivity. The combination of Rust‚Äôs performance with Python‚Äôs flexibility creates a powerful tool for modern development workflows.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>These three tools represent significant advances in their respective domains:</p>
<ul>
<li><strong>RAPTOR RAG</strong> offers a novel approach to document retrieval, though with mixed practical benefits</li>
<li><strong>DFloat11</strong> provides genuine value for LLM deployment with lossless compression</li>
<li><strong>Pyrefly</strong> delivers substantial performance improvements for Python type checking</li>
</ul>
<p>While RAPTOR RAG‚Äôs claims may be overstated, the underlying tree-based approach shows promise for specific use cases like document summarization. DFloat11 and Pyrefly, however, offer clear, measurable benefits that make them valuable additions to any ML or Python development toolkit.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ol type="1">
<li><a href="https://xmad.ai">XMad Team Blog</a> - Meta‚Äôs research team insights</li>
<li><a href="https://arxiv.org/html/2401.18059v1">RAPTOR Paper</a> - Original research paper</li>
<li><a href="https://pyrefly.org/">Pyrefly Official Site</a> - Documentation and installation</li>
<li><a href="https://arxiv.org/abs/2406.02359">DFloat11 Research</a> - Compression technique details</li>
<li><a href="https://docs.llamaindex.ai/">LlamaIndex Documentation</a> - RAG framework integration</li>
</ol>


</section>

 ]]></description>
  <category>blogging</category>
  <category>til</category>
  <category>machine-learning</category>
  <category>python</category>
  <category>rag</category>
  <guid>https://kareemai.com/til/tils/2025-05-25-til.html</guid>
  <pubDate>Sat, 24 May 2025 21:00:00 GMT</pubDate>
  <media:content url="https://kareemai.com/til/tils/images/raptor.png" medium="image" type="image/png" height="47" width="144"/>
</item>
<item>
  <title>Brave Leo AI with Ollama, vllm, and any huggingface llm locally</title>
  <dc:creator>kareem </dc:creator>
  <link>https://kareemai.com/til/tils/2025-05-23-til.html</link>
  <description><![CDATA[ 





<section id="whats-brave-leo-ai-all-about" class="level2">
<h2 class="anchored" data-anchor-id="whats-brave-leo-ai-all-about">What‚Äôs Brave Leo AI All About?</h2>
<p>Brave Leo AI is a super handy, privacy-first AI assistant built right into the Brave browser.</p>
<p>It‚Äôs there to help you out with all sorts of tasks, and it works on your computer (macOS, Windows, Linux) or phone (Android and iOS).</p>
<p>The best part? You don‚Äôt need to sign up or log in to use it for free, and it‚Äôs designed to keep your data private.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://kareemai.com/til/tils/images/brave_leo.png" class="img-fluid figure-img"></p>
<figcaption>Brave Leo with Ollama or VLLM</figcaption>
</figure>
</div>
<section id="what-can-brave-leo-ai-do" class="level3">
<h3 class="anchored" data-anchor-id="what-can-brave-leo-ai-do">What Can Brave Leo AI Do?</h3>
<p>Leo‚Äôs got a lot of tricks up its sleeve: - <strong>Summarize Stuff Instantly</strong>: It can give you quick summaries of webpages, PDFs, Google Docs, Google Sheets, or even YouTube videos by reading their transcripts.</p>
<ul>
<li><p><strong>Answer Questions</strong>: Whether it‚Äôs about a webpage or just something you‚Äôre curious about, Leo can explain things clearly and even offer different perspectives.</p></li>
<li><p><strong>Write and Create</strong>: Need an article, email, essay, or some code? Leo can whip it up for you.</p></li>
<li><p><strong>Translate and Code</strong>: It can translate text into different languages or help with coding by suggesting or generating code snippets.</p></li>
<li><p><strong>Custom AI Models</strong>: With the ‚ÄúBring Your Own Model‚Äù feature, you can plug in your own local or remote AI models for a personalized experience.</p></li>
</ul>
</section>
<section id="is-brave-leo-safe-to-use" class="level3">
<h3 class="anchored" data-anchor-id="is-brave-leo-safe-to-use">Is Brave Leo Safe to Use?</h3>
<p>Privacy is Leo‚Äôs middle name! Here‚Äôs why it‚Äôs safe:</p>
<ul>
<li><p><strong>Anonymized Requests</strong>: Leo uses a reverse proxy, so Brave can‚Äôt tie your requests to your IP address.</p></li>
<li><p><strong>No Chat Storage</strong>: Your conversations aren‚Äôt saved on Brave‚Äôs servers or used to train AI models.</p></li>
<li><p><strong>No Sign-Up Needed</strong>: You can use it for free without an account. Even the premium version uses anonymous tokens to keep things private.</p></li>
<li><p><strong>Local Storage</strong>: Your chat history stays on your device, and you can clear it anytime through the browser settings.</p></li>
<li><p><strong>Heads-Up on Third-Party Models</strong>: If you use external AI models (like Anthropic‚Äôs Claude), their data policies might differ (Claude keeps chats for 30 days, for</p></li>
<li><p>example). Always check the privacy terms if you go that route.</p></li>
</ul>
</section>
<section id="what-about-your-chat-history-with-leo" class="level3">
<h3 class="anchored" data-anchor-id="what-about-your-chat-history-with-leo">What About Your Chat History with Leo?</h3>
<p>If you‚Äôre using Brave version 1.75 or higher on desktop or Android (not in Incognito or Tor mode), you can keep track of your chats with Leo.</p>
<p>They‚Äôre stored locally on your device, not on any server, so you‚Äôre in control. You can revisit, continue, or delete them from the Leo full-page view (brave://leo-ai) or the browser‚Äôs sidebar.</p>
<p>Just note that clearing your browsing history will also wipe out any webpage-related content in your chats. Easy peasy!</p>
</section>
</section>
<section id="bring-your-own-model-byom-with-brave-leo" class="level2">
<h2 class="anchored" data-anchor-id="bring-your-own-model-byom-with-brave-leo">Bring Your Own Model (BYOM) with Brave Leo</h2>
<p>With <a href="https://support.brave.com/hc/en-us/articles/34070140231821-How-do-I-use-the-Bring-Your-Own-Model-BYOM-with-Brave-Leo">BYOM</a> , you can connect your own AI models to Leo for a custom experience. You can use platforms like vLLM, SGLang, or any inefernce engines with any Hugging Face Transformers model, as long as it follows the OpenAI Chat Protocol.</p>
<p>For example, you can run a model like Qwen2.5-VL-3B-Instruct locally with this command:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"></span>
<span id="cb1-2">python <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>m sglang.launch_server <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">--</span>port <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7501</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">--</span>model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>path Qwen<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>Qwen2<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">.5</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>VL<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">B</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>Instruct</span></code></pre></div></div>
<p>This sets up a server for SGLang (or you can use vLLM with a similar command).</p>
<p>Then, in Brave Leo‚Äôs BYOM settings, add your model with these details:</p>
<p><strong>Label</strong>: Qwen2.5-VL-3B-Instruct</p>
<p><strong>Model Request Name</strong>: Qwen2.5</p>
<p><strong>Server Endpoint</strong>: http://127.0.0.1:7501/v1/chat/completions</p>
<p><strong>Context Size</strong>: 4000</p>
<p><strong>API Key</strong>: local</p>
<p><strong>System Prompt</strong>: A custom prompt like, ‚ÄúYou are Leo, a helpful AI assistant by Brave. Provide clear, concise, polite responses under 80 words. Use a neutral tone, clarify if needed, and ensure accuracy.‚Äù</p>
<p>Brave doesn‚Äôt proxy these requests, so check the privacy terms of your chosen provider. Once set up, your model integrates with Leo, letting you use it directly in the browser for tailored, private AI chats. It‚Äôs like giving Leo your own custom brain!</p>


</section>

 ]]></description>
  <category>blogging</category>
  <category>til</category>
  <category>web</category>
  <category>ai</category>
  <category>llm</category>
  <category>local_llm</category>
  <category>ollama</category>
  <guid>https://kareemai.com/til/tils/2025-05-23-til.html</guid>
  <pubDate>Thu, 22 May 2025 21:00:00 GMT</pubDate>
  <media:content url="https://kareemai.com/til/tils/images/brave_leo.png" medium="image" type="image/png" height="71" width="144"/>
</item>
<item>
  <title>ÿßŸÑÿ®ŸàÿµŸÑÿ© ÿßŸÑÿ≠ŸÇŸäŸÇÿ©</title>
  <dc:creator>kareem </dc:creator>
  <link>https://kareemai.com/til/tils/2025-05-21-til.html</link>
  <description><![CDATA[ 





<section id="ÿßŸÑÿ≥ÿπŸä-ÿßŸÑŸä-ÿßŸÑŸÉŸÖÿßŸÑ" class="level2">
<h2 class="anchored" data-anchor-id="ÿßŸÑÿ≥ÿπŸä-ÿßŸÑŸä-ÿßŸÑŸÉŸÖÿßŸÑ">ÿßŸÑÿ≥ÿπŸä ÿßŸÑŸä ÿßŸÑŸÉŸÖÿßŸÑ</h2>
<p>ŸÅŸä ÿßŸÑŸàŸÇÿ™ ÿßŸÑÿ≠ÿßŸÑŸä .. ÿ£ŸÜÿß ÿßŸÇŸàŸÖ ÿ®ÿßŸÑŸÉÿ´Ÿäÿ± ŸÖŸÜ ÿßŸÑŸÖÿ¥ÿßÿ±Ÿäÿπ ŸàŸÖŸÜŸáÿß ÿßŸÑŸàÿßÿπÿØ ÿßŸÑÿ∞Ÿä ÿ≥ŸäŸÉŸàŸÜ ÿ≥ÿ®ÿ® ŸÅŸä ÿ•ÿØÿÆÿßŸÑ ÿØÿÆŸÑ ŸÖÿßÿØŸä ŸÉÿ®Ÿäÿ± Ÿà ÿπŸÑŸÖŸä ÿ•ŸÜ ÿ¥ÿßÿ° ÿßŸÑŸÑŸá.</p>
<p>ÿ≠ÿßŸÑŸäÿß ÿ£ÿπŸÖŸÑ ÿπŸÑŸä ÿ±ÿ≥ÿßŸÑÿ© ÿßŸÑŸÖÿßÿ≥ÿ™ÿ± + ÿπÿ∂Ÿà ŸÅÿπÿßŸÑ ŸÅŸä ŸÖÿ¨ÿ™ŸÖÿπŸäŸÜ ŸÑÿ™ÿ∑ŸàŸäÿ± ÿßŸÑÿ®ÿ±ŸÖÿ¨Ÿäÿßÿ™ ŸàÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿ•ÿµÿ∑ŸÜÿßÿπŸä ŸÅŸä ÿßŸÑÿπÿßŸÑŸÖ ÿßŸÑÿπÿ±ÿ®Ÿä ŸàÿßŸÑÿ•ÿ≥ŸÑÿßŸÖŸä ŸàŸÑŸÑŸá ÿßŸÑÿ≠ŸÖÿØ ŸàÿßŸÑŸÅÿ∂ŸÑ Ÿàÿ≠ÿØŸá + ÿ£ÿπŸÖŸÑ ŸÅŸä ŸÖÿπŸÖŸÑ ÿ®ÿßÿ≠ÿ´Ÿä ÿ™ÿßÿ®ÿπ ŸÑÿßÿ≠ÿØ ÿßŸÑÿ£ÿ¥ÿÆÿßÿµ ÿßŸÑŸÖŸÖŸäÿ≤ÿ© ÿ¨ÿØÿß Ÿà ŸÑÿØŸä ÿ£ÿπŸÖÿßŸÑ ÿ®ÿßÿ≠ÿ´ŸäŸá ŸàŸÖÿ¥ÿßÿ±Ÿäÿπ ÿÆÿßÿµŸá ŸÖŸÜ ŸÜŸàÿßÿ≠Ÿä ŸÖÿ™ÿπÿØÿØÿ©.</p>
<p>ÿ£ÿ≥ÿ™ÿ¥ÿπÿ± ÿ≠ÿßŸÑŸäÿß ŸÉÿ´ÿ±ÿ© ÿßŸÑŸÖŸáÿßŸÖ ŸàÿßŸÑÿ™ÿ¥ÿ™ÿ™ ÿßŸÑŸÉÿ®Ÿäÿ± Ÿà ŸÇŸÑÿ© ÿßŸÑŸàŸÇÿ™ ŸàÿßŸÑŸÖÿ¨ŸáŸàÿØ ŸàÿßŸÑŸÉÿ´Ÿäÿ± ..ŸÑŸÉŸÜ Ÿáÿ∞ÿß ŸÑŸäÿ≥ ÿßŸÑŸÖÿ¥ŸÉŸÑÿ©. ŸÖÿß ÿ£ÿ≥ÿ™ÿ¥ÿπÿ±Ÿá ŸáŸà ÿ≤ŸäÿßÿØÿ© ÿßŸÑÿ•ÿπÿ¨ÿßÿ® ÿ®ÿßŸÑŸÜŸÅÿ≥ ŸàÿßŸÑÿ®ÿπÿØ ÿπŸÜ ÿßŸÑÿπÿ®ÿßÿØÿ© ŸÖŸÜ ÿ£ÿ¨ŸÑ ŸÑÿ∞ÿ© ÿßŸÑÿπŸÑŸÖ Ÿà ÿ≠ÿ∏ ÿßŸÑŸÜŸÅÿ≥.</p>
<p>Ÿà ŸÖŸÜ ÿØÿßÿÆŸÑŸä ŸÑÿß ÿ£ŸÇŸÜÿπ ÿ®ŸÉŸÑ ŸÖÿß ÿ£ŸÇÿØŸÖ ..Ÿàÿßÿ±Ÿä ÿßŸÜŸá ŸáŸÜÿßŸÉ ÿßŸÑŸÖÿ≤ŸäÿØ ŸÖŸÜ ÿßŸÑÿπŸÑŸÖ ŸàÿßŸÑŸÖÿπÿ±ŸÅÿ© Ÿäÿ¨ÿ® ÿ™ÿ≠ÿµŸÑŸäŸáÿß Ÿà ÿπÿØŸÖ ÿßŸÑÿ±ÿ∂ÿß ÿ®ŸÉŸàŸÜŸä ŸÖÿ¨ÿ±ÿØ ŸÖÿ≥ÿ™ŸÇÿ®ŸÑ ŸàŸÖÿπÿØŸÑ ŸÑŸÖÿß ÿ£ÿ≥ÿ™ŸÇÿ®ŸÑŸá. ŸÖÿ¨ÿßŸÑ ÿØÿ±ÿßÿ≥ÿ™Ÿä ŸáŸà ÿßŸÑÿ∞ŸÉÿßÿ° ŸàÿßŸÑÿ•ÿµÿ∑ŸÜÿßÿπŸä ÿ®ÿ™ÿÆÿµÿµÿßÿ™Ÿá Ÿà ÿπŸÑŸàŸÖ ÿßŸÑÿ≠ÿßÿ≥ÿ®.</p>
<p>ŸÖÿß Ÿäÿ≤ÿπÿ¨ŸÜŸä ÿßŸÑŸÉÿ´Ÿäÿ± ..ŸÖŸÜŸáÿß ŸÑŸÖÿßÿ∞ÿß ÿ£ŸÇŸàŸÖ ÿ®ÿßÿ≥ÿ™ŸÇÿ®ÿßŸÑ ÿßŸÑÿπŸÑŸÖ ŸÖŸÜ ÿßŸÑÿÆÿßÿ±ÿ¨ ŸàŸÑÿ≥ÿ™ ÿ£ÿ™ÿ≠ÿØÿ´ ÿπŸÜ ŸÖÿ¨ÿ±ÿØ ÿßŸÑÿ¥ÿ±Ÿàÿ≠ÿßÿ™ ÿßŸÑÿ£ÿ¨ŸÜÿ®Ÿäÿ© Ÿà ÿßŸÑÿ£Ÿàÿ±ÿßŸÇ ŸàÿßŸÑŸÖÿ¥ÿßÿ±Ÿäÿπ ÿßŸÑÿ®ÿ±ŸÖÿ¨Ÿäÿ© ŸàŸÉŸÑ Ÿáÿ∞ÿß ŸàŸÑŸÉŸÜ ÿ£ÿ™ÿ≠ÿØÿ´ ÿπŸÜ ÿßŸÑÿ±ŸäÿßÿØÿ© ŸÅŸä ÿßŸÑÿπŸÑŸÖ ÿ∞ÿßÿ™Ÿá. ÿ£ÿ™ÿ≠ÿØÿ´ ÿπŸÜ ÿ£ŸÖÿ´ÿßŸÑ ÿ¨ŸäŸÅÿ±Ÿä ŸáŸäŸÜÿ™ŸàŸÜ ŸàŸÖÿß ÿ®ÿπÿØŸá ..ÿßŸÑÿ±ÿ∫ÿ®ÿ© ŸÅŸä ÿßŸÑŸàÿµŸàŸÑ ÿßŸÑŸä ÿ≠ÿßŸÅÿ© ÿßŸÑÿπŸÑŸÖ ŸàÿßŸÑÿ•ÿ®ÿØÿßÿπ ŸàŸÑŸäÿ≥ ŸÖÿ¨ÿ±ÿØ ÿßŸÑÿ™ÿπÿØŸäŸÑ ŸàŸÅŸáŸÖ ŸÖÿß ŸäÿµÿØÿ±ŸàŸÜ.</p>
<p>ŸáŸÑ ÿ£ŸÉÿ™ŸÅŸä ÿ®Ÿáÿ∞ÿß !</p>
<p>ŸÅŸä ÿßŸÑÿ≠ŸÇŸäŸÇÿ© ŸÑÿß ÿßÿ™ÿ∞ŸÉÿ± ÿπŸÜÿØŸÖÿß ÿ™ŸÖ ÿ•ÿ∑ŸÑÿßŸÇ deepseek ŸàŸÇÿ±ÿßÿ™ ÿπŸÜ ÿßŸÑÿ¥ÿ±ŸÉÿ© ÿßŸÑÿ™Ÿä ÿ™ŸÇŸÅ Ÿàÿ±ÿßÿ¶Ÿáÿß..ŸÑŸÖÿßÿ∞ÿß ŸÑŸÖ ÿ™ŸÉÿ±ÿ± ÿßŸÑÿ™ÿ¨ÿ±ÿ®ÿ© ÿπÿ±ÿ®Ÿäÿ© ÿ•ÿ≥ŸÑÿßŸÖŸäÿ© ÿÆÿßŸÑÿµÿ©! ŸÑŸäÿ≥ ŸÖÿ≥ÿ™ÿ≠ŸäŸÑ ÿ™ŸÇŸÜŸäÿß ŸàŸÜÿ≠ŸÜ ÿ£ŸàŸÑŸä ÿ®ŸÑÿ∫ÿ™ŸÜÿß Ÿàÿ™ÿ±ÿßÿ´ŸÜÿß ŸÖŸÜ ÿ∫Ÿäÿ±ŸÜÿß ŸÖŸÜ ÿßŸÑÿ¥ÿπŸàÿ® ÿÆÿµŸàÿµÿß ..ÿ£ŸÜ ÿ™ŸÉŸàŸÜ Ÿáÿ∞Ÿá ÿßŸÑÿ¥ÿπŸàÿ® ŸÑÿØŸäŸáÿß ŸÉÿ±ÿßŸáŸäÿ© ŸÖÿ™ÿ¥ÿπÿ®Ÿá ÿÆÿßÿµÿ© ŸÑÿØŸäŸÜŸÉ Ÿàÿ¥ÿπÿ®ŸÉ ŸàŸÑŸÜÿß ŸÅŸä ŸÅŸÑÿ≥ÿ∑ŸäŸÜ ÿπÿ®ÿ±ÿ© ŸÅŸäŸÖÿß Ÿäÿ≠ÿØÿ´ ŸÅŸäŸáÿß ŸÖŸÜ ÿ™ŸÜŸÉŸäŸÑ Ÿà ÿ¨ÿ±ÿßÿ¶ŸÖ ŸÑÿß ÿßÿ≠ÿ® ÿ≠ÿ™Ÿä ÿßŸÑÿ™ŸÅŸÉÿ± ŸÅŸäŸáÿß. ŸÑŸÉŸÜ ŸÖÿß Ÿäÿ≤ŸäÿØ ÿßŸÑÿ≠ÿ≤ŸÜ ŸàÿßŸÑÿ∂ÿπŸÅ ŸàÿßŸÑŸáŸÖ ŸàÿßŸÑÿ∫ŸÖÿ© ..ÿ£ŸÜ ÿ™ÿ¨ÿØ ŸÖŸÜ ÿßŸÑÿ±ŸäÿßÿØÿ© ŸÅŸä ŸÖÿ¨ÿßŸÑŸÉ Ÿà ÿ™ÿÆÿµÿµŸÉ ÿ£ÿ∫ŸÑÿ®ŸáŸÖ ŸÖŸÜ ŸáŸàŸÑÿßÿ° ÿßŸÑÿµŸÜŸÅ ÿßŸÑÿ∞Ÿä ŸäŸÇÿ™ŸÑ ÿ•ÿÆŸàÿ™ŸÉ ÿ®ÿØŸÖ ÿ®ÿßÿ±ÿØ!</p>
<p>ŸÉŸäŸÅ ÿ™ÿ≥ÿπÿØ ŸÜŸÅÿ≥ŸÉ ŸàÿßŸÜÿ™ ÿ™ÿßÿ®ÿπ ŸÑŸáŸàŸÑÿßÿ° ÿ™ŸÜÿ™ÿ∏ÿ± ŸÖŸÜŸáŸÖ ÿ£ŸÜ Ÿäÿπÿ∑ŸàŸÉ ÿ®ÿπÿ∂ ÿßŸÑŸÅÿ™ÿ™ÿßÿ™ ŸÖŸÜ ÿßŸÑÿπŸÑŸÖ ŸÑÿ™ÿ∞Ÿáÿ® ŸÑÿ™ŸÜÿ™ŸÅÿÆ ÿ®Ÿá ŸÜŸÅÿ≥ŸÉ Ÿàÿ™ÿ∏ŸÜ ÿ£ŸÜ ŸÑÿØŸäŸÉ ÿ¥Ÿä ŸÖŸÜ ÿπŸÑŸÖ ŸàŸáŸà ŸÅÿ™ÿßÿ™.</p>
<p>ÿ®ÿØÿßŸäÿ© ŸÖŸÜ ÿßŸÑÿ®ÿßÿ≠ÿ´ŸäŸÜ ŸÑÿ£ÿµÿ≠ÿßÿ® ÿßŸÑÿ¥ÿ±ŸÉÿßÿ™ ŸÑŸÑŸÖŸàÿßŸÇÿπ ÿßŸÑÿ™Ÿä ÿ£ŸÇŸàŸÖ ÿ®ÿßŸÑÿßÿ≥ÿ™ÿ∂ÿßŸÅÿ© ÿπŸÑŸäŸáÿß ÿ•ŸÑŸä ÿßŸÑŸÉŸàÿ±ÿ≥ÿßÿ™ Ÿà ŸÖÿ¨ÿ™ŸÖÿπÿßÿ™ ÿßŸÑÿ™ŸÇŸÜŸäÿ©..ÿ®ŸÑ ÿ™ÿ≤ŸäÿØ ÿßŸÑÿ≠ÿ≥ÿ±ÿ© ÿπŸÜÿØŸä ÿ®ÿ≠ÿ´Ÿä ŸÅŸä ÿ®ŸäÿßŸÜÿßÿ™ ŸÑÿ™ÿ∑ŸàŸäÿ± ŸÜŸÖŸàÿ∞ÿ¨ ÿµÿ∫Ÿäÿ± ŸÑŸÖŸáŸÖÿ© ŸÖŸÜ ŸÖŸáÿßŸÖ ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿ£ÿ¨ÿØ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ£ŸäŸÜ !ŸàŸÖŸÜ ŸäÿπŸÖŸÑ ÿπŸÑŸäŸáÿß !</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://kareemai.com/til/tils/images/isreal.png" class="img-fluid figure-img"></p>
<figcaption>ÿßŸÑÿ¥ÿπŸàÿ± ÿ®ÿßŸÑÿπÿ¨ÿ≤</figcaption>
</figure>
</div>
<p>ŸäŸÇŸàŸÖŸàŸÜ ÿ®ÿ™ÿ¨ŸÖŸäÿπ Ÿà ÿ™ÿ∑ŸàŸäÿ± ŸÜŸÖÿßÿ∞ÿ¨ ÿ™ÿÆÿØŸÖ ŸÅŸáŸÖ ÿßŸÑŸÑŸáÿ¨ÿ© ÿßŸÑÿ¥ÿßŸÖŸäÿ© ÿ™ÿ≥ÿßÿπÿØŸáŸÖ ŸÅŸä ŸÉŸÑ ÿ£ÿπŸÖÿßŸÑŸáŸÖ..ŸàŸÑŸäÿ≥ Ÿáÿ∞ÿß ŸàŸÇÿ™ ÿßŸÑÿ≠ÿØŸäÿ´..</p>
<p>ÿßŸÑÿπŸàÿßÿ¶ŸÇ ŸÉÿ´Ÿäÿ± ŸÖÿßÿØŸäÿ© Ÿà ŸÖÿπÿ±ŸÅŸäÿ© ŸàŸÉÿ´Ÿäÿ± ŸÑŸÉŸÜ ÿ≥ŸÇŸÅ ÿßŸÑŸÖŸÖŸÉŸÜ ŸÖÿ∞ŸáŸÑ..Ÿà ŸáŸÜÿßŸÉ ÿ£ŸÖŸÑ Ÿà ÿ•ŸÜ ŸÑŸÖ ŸäŸÉŸÜ ŸäŸÉŸÅŸä ÿ£ŸÜ ÿ™ŸÖŸàÿ™ ŸàÿßŸÜÿ™ ÿ™ÿ≠ÿßŸàŸÑ Ÿàÿ™ŸÜŸÉÿ± ÿ®ŸÇŸÑÿ®ŸÉ ŸàŸáÿ∞ÿß ÿ£ÿ∂ÿπŸÅ ÿßŸÑÿ•ŸäŸÖÿßŸÜ.</p>
</section>
<section id="ÿßŸÑÿ®ÿØÿßŸäÿ©" class="level2">
<h2 class="anchored" data-anchor-id="ÿßŸÑÿ®ÿØÿßŸäÿ©">ÿßŸÑÿ®ÿØÿßŸäÿ©</h2>
<p>ŸÑÿØŸä ŸÖŸÜ ÿßŸÑÿπŸÖÿ± ÿßŸÑÿ¢ŸÜ 22 ÿπÿßŸÖ ŸÑÿØŸä ÿßŸÑŸÉÿ´Ÿäÿ± ŸÖŸÜ ÿßŸÑŸÜÿØŸÖ ŸàÿßŸÑÿ≠ÿ≥ÿ±ÿ© ÿπŸÑŸä ÿßŸÑÿπŸÖÿ± ÿßŸÑŸÅÿßÿ¶ÿ™ ŸÅŸä ÿ∫Ÿäÿ± ÿßŸÑÿ∑ÿßÿπÿ© ŸàÿßŸÑÿπŸÖŸÑ ÿßŸÑÿµÿßŸÑÿ≠ ŸàŸàÿØÿØÿ™ ŸÑŸà ÿπŸÜÿØŸä ŸÅÿ±ÿµÿ© ŸÅŸä ÿßŸÑÿ±ÿ¨Ÿàÿπ ŸÑŸÑÿÆŸÑŸÅ ŸÉÿ´Ÿäÿ±ÿß ŸàÿßŸÑÿ®ÿØÿßŸäÿ© ŸÖÿ¨ÿØÿØÿß..ŸÑŸÉŸÜŸÜÿß ŸÑÿ≥ŸÜÿß ŸÅŸä ŸÅŸäŸÑŸÖ ÿ£ŸÜŸÖŸä ÿ®ŸÑ ŸàÿßŸÇÿπ‚Ä¶ÿπÿ≤ÿßÿ¶Ÿä ÿ•ŸÜŸÉ ÿ•ŸÜ ÿ™ÿ≥ÿ™ÿØÿ±ŸÉÿå ÿ™ÿ≥ÿ™ÿØÿ±ŸÉ ÿπŸÜÿØ ÿ±ÿ® ŸÉÿ±ŸäŸÖ ŸÇÿßÿØÿ± ÿπŸÑŸä ŸÉŸÑ ÿ¥ÿ¶. Ÿäÿ®ÿØŸÑ ŸÑŸÉ ÿ≥Ÿäÿ¶ÿßÿ™ŸÉ.</p>
<p>ÿ£ÿπÿßŸÜŸä ŸÖŸÜ ÿßŸÑŸÉÿ´Ÿäÿ± ŸÖŸÜ ÿπÿ≥ÿ± ÿßŸÑŸÅŸáŸÖ ŸÅŸä ŸÉŸÑ ÿ¥ÿ¶ ÿ™ŸÇÿ±Ÿäÿ®ÿß..ÿÆÿµŸàÿµÿß ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ŸÑÿØŸä ÿπÿ≥ÿ± ŸÇÿ±ÿßÿ°ÿ© Ÿà ŸÉÿ™ÿßÿ®ÿ© Ÿà ÿßŸÑŸä ÿßŸÑÿµŸÅ ÿßŸÑÿ´ÿßŸÑÿ´ ÿßŸÑÿ´ÿßŸÜŸàŸä ŸÑÿØŸä ŸÖÿ¥ÿßŸÉŸÑ ŸÅÿßÿØÿ≠Ÿá ŸÅŸä ÿßŸÑŸÉÿ™ÿßÿ®ÿ©..ÿßŸÑÿ• ÿ£ŸÜŸÜŸä ŸÉŸÜÿ™ ÿßÿ≠ÿ® ÿßŸÑÿ±Ÿäÿßÿ∂Ÿäÿßÿ™ ÿ®ÿ±ÿ∫ŸÖ ŸÅÿ¥ŸÑŸä ŸÅŸäŸáÿß ..Ÿàÿßÿ≠ÿ® ÿßŸÑÿ•ŸÜÿ¨ŸäŸÑŸäÿ≤Ÿä ŸàÿßŸÑÿ≠ÿßŸÑ ÿ£ÿ≠ÿ≥ŸÜ ŸÖŸÜ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿ®ŸÉÿ´Ÿäÿ± ..ÿßÿ≠ÿ®ŸáŸÖ ŸàŸÑŸÉŸÜŸáŸÖ ŸÑÿß Ÿäÿ≠ÿ®ŸàŸÜŸä ŸàÿßŸÑÿ≠ŸÖÿØ ŸÑŸÑŸá.</p>
<p>ÿ®ÿØÿßÿ™ ÿ£ÿ≠ÿßŸàŸÑ ÿ¨ÿßŸáÿØÿß ŸÅŸä ÿßŸÑÿµŸÅ ÿßŸÑÿ´ÿßŸÜŸä ÿßŸÑÿ´ÿßŸÜŸàŸä ŸÖÿ≠ÿßŸàŸÑÿßÿ™ ÿ™ÿπÿ™ÿ®ÿ± ÿ¨ÿßÿØŸá ÿßŸÑŸä ÿßŸÑÿ´ÿßŸÑÿ´ ÿßŸÑÿ´ÿßŸÜŸàŸä ŸàŸÉŸÜÿ™ ŸÅÿßÿ¥ŸÑŸá Ÿàÿ™ÿ¨ÿßŸàÿ≤ÿ™ ÿßŸÑÿßŸÖÿ™ÿ≠ÿßŸÜÿßÿ™ ÿ®ÿßŸÑÿ∫ÿ¥ ŸàÿØÿÆŸÑÿ™ ÿßŸÑÿ¨ÿßŸÖÿπÿ© ŸàŸÑÿß ÿßÿ≥ÿ™ÿ≠ŸÇ Ÿáÿ∞ÿß ÿßŸÑŸÖŸÉÿßŸÜ ŸàŸÑÿß ÿ∞ŸÑŸÉ ÿßŸÑŸÖÿ¨ŸÖŸàÿπ.</p>
<p>ŸÅŸä ŸÉŸÑ ÿ™ŸÑŸÉ ÿßŸÑÿ≥ŸÜŸàÿßÿ™ ÿßŸÑŸÅÿßÿ¶ÿ™Ÿá ŸÉŸÜÿ™ ÿ™ÿπŸäÿ¥ ŸÅŸä ÿπÿßŸÑŸÖ ÿßŸÑŸÖŸÇÿßÿ±ÿßŸÜÿßÿ™ Ÿàÿ™ŸÜÿ™ÿ∏ÿ± ŸÖŸÜ ÿßŸÑÿ£ŸáŸÑ Ÿà ÿßŸÑÿ£ÿµÿ≠ÿßÿ® ÿßŸÑÿ•ÿπÿ™ÿ±ÿßŸÅ ŸàÿßŸÑÿ™ŸÇÿØŸäÿ± ..ÿ®ÿ±ÿ∫ŸÖ ŸÉŸÑ Ÿáÿ∞ÿß ŸÅŸÉŸÜÿ™ ÿ£ÿ™ŸÑŸÇŸä ÿßŸÑŸÉÿ´Ÿäÿ± ŸÖŸÜ ÿßŸÑÿ≠ÿ® ŸàÿßŸÑÿ•ŸäŸÖÿßŸÜ ÿ®Ÿä ŸÖŸÜ ŸÉŸÑ ŸÖŸÜ ÿØÿ±ÿ≥ ŸÑŸä ÿ™ŸÇÿ±Ÿäÿ®ÿß. ŸÅÿ¨ÿ≤ÿßŸáŸÖ ÿßŸÑŸÑŸá ÿÆŸäÿ±ÿß.</p>
<p>ŸÖŸÜ ÿØÿßÿÆŸÑŸä ŸÑÿ£ ÿßŸÅŸÉÿ± ŸÅŸä Ÿáÿ∞Ÿá ÿßŸÑÿ£ŸÅŸÉÿßÿ± ŸÉÿ´Ÿäÿ±ÿß..ŸÑŸÉŸÜŸä ŸÅŸä ÿ®ÿØÿßŸäÿ© ÿßŸÑÿ¨ÿßŸÖÿπÿ© ÿ™ÿ®ÿØŸÑ ÿßŸÑÿ≠ÿßŸÑ Ÿà ÿ£ÿµÿ®ÿ≠ ŸÉŸÑ ŸàŸÇÿ™Ÿä ÿ™ÿπŸÑŸÖ ŸàÿØÿ±ÿßÿ≥ÿ© ÿßŸÑŸä ÿ£ŸÜ ÿ™ÿÆÿ±ÿ¨ÿ™ ÿßŸÑÿπÿßŸÖ ÿßŸÑŸÅÿßÿ¶ÿ™ Ÿà ŸÖÿ±ÿ±ÿ™ ÿ®ŸÉÿ´Ÿäÿ± ŸÖŸÜ ÿßŸÑÿµÿπÿßÿ® Ÿàÿ™ÿ¨ÿßŸàÿ≤ÿ™Ÿáÿß Ÿà ÿßŸÑÿ≠ŸÖÿØ ŸÑŸÑŸá Ÿà ÿßŸÑŸä ÿßŸÑÿ£ŸÜ ÿ£ŸÇÿ™ÿ±ÿ® ŸÖŸÜ ÿßŸÑŸÑÿ≠ÿ∏ÿ© ÿßŸÑÿ™Ÿä ÿ≥Ÿäÿ™ÿ∫Ÿäÿ± ŸÅŸäŸáÿß ŸÉŸÑ ÿ¥ÿ¶ ŸàŸäÿ™ÿ®ÿØŸÑ ŸÉŸÑ ÿßŸÑÿ≠ÿßŸÑ..ŸÖŸÜ ŸÅÿ±ÿµ ÿßŸÑÿ≥ŸÅÿ± ŸàÿßŸÑŸÖŸÜÿ≠ Ÿà ŸÜÿ¨ÿßÿ≠ ÿßŸÑÿ£ÿπŸÖÿßŸÑ ŸàÿßŸÑÿ≠ÿ±Ÿäÿ© ÿßŸÑŸÖÿßÿØÿ© Ÿà ÿßŸÑŸÉÿ´Ÿäÿ± ÿ´ŸÖ ŸÇÿ®ŸÑŸáÿß ŸäŸàŸÖ ÿßŸÑÿ≠ÿµÿßÿØ ÿ£Ÿà ŸäŸàŸÖŸáÿß ŸÖÿ´ŸÑ ÿ£ÿÆÿ± ŸÖÿ±Ÿá..ÿ£ŸÅŸÇÿØ ŸÉŸÑ Ÿáÿ∞ÿß Ÿà ÿ£ÿ®ÿØÿß ŸÖŸÜ ÿ¨ÿØŸäÿØ..ÿ™ÿ¥ÿπÿ± ÿ®ÿßŸÑÿ≠ÿ≤ŸÜ ŸÉÿ´Ÿäÿ±ÿß..ŸÑŸÉŸÜ ÿ®ÿπÿØ ŸÅÿ™ÿ±ÿ© ÿ™ÿ¨ÿØ ÿ±ÿ≠ŸÖÿßÿ™ ÿ±ÿ®ŸÉ Ÿà ÿ™ŸÇÿØŸäÿ±Ÿá ŸÅŸä ŸÉŸÑ Ÿáÿ∞ÿß Ÿà ŸáŸÜÿßŸÉ ÿ£ŸàŸÇÿßÿ™ ŸÑÿß ÿ™ÿπÿ±ŸÅ ŸÖÿßŸáŸä ÿßŸÑÿ≠ŸÉŸÖÿ© ŸÑŸÉŸÜ ÿ™ÿ®ÿØÿß ŸÅŸä ÿ±ÿ§Ÿäÿ© ÿ™ŸÇÿµŸäÿ±ŸÉ Ÿà ÿ™ÿπŸäÿØ ÿ≠ÿ≥ÿßÿ®ÿßÿ™ Ÿàÿ™ÿ™Ÿàÿ¨Ÿá ÿßŸÑŸä ÿßŸÑŸÑŸá ŸÖÿ¨ÿØÿØÿß.</p>
</section>
<section id="ÿßŸÑÿ™ÿ≠ÿ±ÿ±-ŸÖŸÜ-ÿßŸÑÿØŸÜÿ≥" class="level2">
<h2 class="anchored" data-anchor-id="ÿßŸÑÿ™ÿ≠ÿ±ÿ±-ŸÖŸÜ-ÿßŸÑÿØŸÜÿ≥">ÿßŸÑÿ™ÿ≠ÿ±ÿ± ŸÖŸÜ ÿßŸÑÿØŸÜÿ≥</h2>
<p>ÿ®ÿπÿØ Ÿáÿ∞Ÿá ÿßŸÑÿ≥ŸÜŸäŸÜ ÿ™ŸÜÿ∏ÿ± ŸÑŸÜŸÅÿ≥ŸÉ Ÿà ÿ™ÿ®ÿØÿß ŸÅŸä ÿßŸÑÿ∫Ÿàÿµ ŸÅŸä ÿ£ÿπŸÖÿßŸÉ ÿßŸÑÿ∞ÿßÿ™ ŸÑŸÖÿßÿ∞ÿß ÿ£ŸÇŸàŸÖ ÿ®Ÿáÿ∞ÿß ŸàŸÖÿßŸáŸä ÿßŸÑÿ±ÿ∫ÿ®ÿ© ŸàÿßŸÑŸáÿØŸÅ !</p>
<p>ÿ™ÿ¨ÿØ ÿ£ŸÜŸÉ ÿ™ŸÖ ÿ≠ÿ®ÿ≥ŸÉ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÖÿπÿ™ŸÇÿØÿßÿ™ Ÿà ÿ£ŸÅŸÉÿßÿ± ÿ≥ÿßŸÖÿ© Ÿà ŸÅÿßÿ≥ÿØÿ© ÿ∑ŸäŸÑÿ© ÿ≠Ÿäÿßÿ™ŸÉ Ÿà ÿßŸÑŸÇŸÑŸäŸÑ ÿßŸÑŸÇŸÑŸÑŸäŸÑ ŸÖŸÜ ÿßŸÑÿ£ŸÅŸÉÿßÿ± ÿßŸÑÿµÿßŸÑÿ≠ÿ© ŸÖŸÜ ÿßŸÑÿ™ÿ±ÿ®Ÿäÿ© Ÿà ÿßŸÑŸÖÿ¨ÿ™ŸÖÿπ‚Ä¶ÿ™ÿ£ÿ™Ÿä Ÿáÿ∞Ÿá ÿßŸÑÿ£ŸÅŸÉÿßÿ± ÿ®ÿπÿØ ÿ£ŸÜ ÿ™ŸÅŸÉŸÉÿ™ ÿßŸÑÿ≠Ÿäÿßÿ© Ÿàÿ™ÿ®ÿØŸÑÿ™ ŸàŸáŸÜÿßŸÉ ÿßŸÑŸÉÿ´Ÿäÿ± ŸÖŸÜ ÿßŸÑŸÖŸàÿ™Ÿä ŸÅŸä ÿ≠Ÿäÿßÿ™ŸÉ Ÿàÿ£ŸÜÿ™ ŸÖÿßÿ≤ŸÑÿ™</p>
<p>ÿ™ÿπŸäÿ¥ ŸÅŸä ÿ≥ÿπŸäŸÉ ŸÑŸÑÿßÿπÿ™ÿ±ÿßŸÅ ÿ®ŸÉ‚Ä¶ÿ™ÿπŸäÿ¥ ÿ£ÿ≥Ÿäÿ± ŸÑŸÑŸÖÿßÿ∂Ÿä ŸÅÿ™ÿ∏ŸÑŸÖ ÿ®ÿ∞ŸÑŸÉ ÿßŸÑÿ≠ÿßÿ∂ÿ± ŸàÿßŸÑŸÖÿ≥ÿ™ŸÇÿ®ŸÑ.</p>
</section>
<section id="ÿßŸÑÿ£ŸÜÿßŸÜŸäÿ©-ŸàŸÑÿ∞ÿ©-ÿßŸÑÿπŸÑŸÖ" class="level2">
<h2 class="anchored" data-anchor-id="ÿßŸÑÿ£ŸÜÿßŸÜŸäÿ©-ŸàŸÑÿ∞ÿ©-ÿßŸÑÿπŸÑŸÖ">ÿßŸÑÿ£ŸÜÿßŸÜŸäÿ© ŸàŸÑÿ∞ÿ© ÿßŸÑÿπŸÑŸÖ</h2>
<p>ŸÉÿßŸÜ ŸáŸÜÿßŸÉ ÿ£ŸàŸÇÿßÿ™ Ÿäÿ¨ÿ® ÿ£ŸÜ ÿßÿÆÿ™ÿßÿ± ÿßŸÑŸÖÿßŸÑ ÿ£ŸÖ ÿßŸÑÿπŸÑŸÖ!ŸàŸÉŸÜÿ™ ÿ®ŸÉŸÑ ÿ®ÿ≥ÿßÿ∑ÿ© ÿßŸÑÿπŸÑŸÖ ..ŸÅÿßŸÑŸÖÿßŸÑ ÿ≥Ÿäÿ£ÿ™Ÿä ŸÖÿ≥ÿ™ŸÇÿ®ŸÑÿß ÿ®ÿπÿØ ÿßŸÑÿπŸÑŸÖ ..ŸàŸáÿ∞ÿß ŸÑŸäÿ≥ ÿµÿ≠Ÿäÿ≠ÿß 100% ŸàŸÑŸÉŸÜ ÿßŸÑŸàÿßŸÇÿπ ŸÖÿÆÿ™ŸÑŸÅ..</p>
<p>ŸÑŸÖÿßÿ∞ÿß ÿ™ÿ±ŸäÿØ ÿßŸÑÿπŸÑŸÖ!! ÿ£ŸÇŸàŸÖ ÿ®ÿ≥ÿ±ÿØ ÿßŸÑŸÖÿßÿ¶ÿßÿ™ ŸÖŸÜ ÿßŸÑÿ¥ÿπÿßÿ±ÿßÿ™ ÿßŸÑÿ¨ÿ∞ÿßÿ®ÿ© ÿπŸÜ ÿßŸÑÿπŸÑŸÖ ŸàÿßŸÑÿ™ÿπŸÑŸÖ ŸàÿßŸÑŸáŸÖÿ© Ÿàÿ±ŸÅÿπ ÿßŸÑÿ≠ÿ±ÿ¨ ÿπŸÜ ÿßŸÑÿ£ŸÖÿ© ŸàÿßŸÑÿ´ÿ∫ÿ± ŸàŸÉŸÑÿßŸÖ Ÿäÿ¨ÿπŸÑŸÉ ÿ™ŸÇŸàŸÑ ŸäÿßŸá ÿ®ÿßÿ±ŸÉ ÿßŸÑŸÑŸá ŸÅŸäŸÉ‚Ä¶</p>
<p>ŸÖÿπÿ∏ŸÖŸá ŸÉŸÑÿßŸÖ ..ŸÑÿ≠ÿ∏ÿßÿ™ ÿßŸÑÿ≠ŸÇŸäŸÇÿ© ŸáŸä ÿßŸÑŸÖÿ≠ŸÜ ..ÿßŸÑŸÑÿ≠ÿ∏ÿ© ÿßŸÑÿ™Ÿä ÿ≥ŸàŸÅ ÿ™ÿ™ÿÆŸÑŸä ÿπŸÜ ÿπŸÖŸÑ ŸÖÿπŸäŸÜ Ÿàÿ≥ÿπŸä Ÿà ŸÑŸÜ Ÿäÿ∞ŸÉÿ± ÿßÿ≥ŸÖŸÉ ŸàŸÑŸÉŸÜŸá ÿ≥ŸäÿÆÿ±ÿ¨ ÿ®ÿßÿ≥ŸÖ ŸÉŸÑŸä ŸàŸÑŸäÿ≥ ÿßÿ≥ŸÖ ŸäÿÆÿµŸÉ ŸÅÿ™ÿ¥ÿπÿ± ŸÖŸÜ ÿØÿßÿÆŸÑŸÉ ŸÖÿßÿ∞ÿß ŸÑŸà ŸÉŸÜÿ™ ÿßŸÜÿß ŸÖŸÜ ŸÇÿßŸÖ ÿ®Ÿá Ÿàÿ≠ÿØŸä Ÿàÿ™ŸÖ ÿ∞ŸÉÿ±Ÿä ÿßÿ≥ŸÖŸä Ÿàÿ≠ÿØŸä ÿπŸÑŸä ŸÖŸÜÿµÿßÿ™Ÿä ÿ≥ŸàŸÅ ÿ£ÿµÿ®ÿ≠ ŸÖÿπÿ±ŸàŸÅÿß ŸÅŸä ŸÖÿ¨ÿßŸÑŸä Ÿà ÿßÿ≠ÿµŸÑ ÿπŸÑŸä ŸÅÿ±ÿµÿ© ÿπŸÖŸÑ ÿ¨ŸäÿØÿ©‚Ä¶.</p>
<p>ŸÑŸÜ ÿ£ÿÆÿ®ÿ± Ÿáÿ∞ÿß ŸÉŸäŸÅŸäÿ© ÿπŸÖŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ Ÿáÿ∞ÿß ÿßŸà ÿ£ŸÜ ŸáŸÜÿßŸÉ ÿ∑ÿ±ŸäŸÇÿ© ÿ£ÿ≥ŸáŸÑ ÿßŸÑŸä ÿßŸÜ ÿßŸÇŸàŸÖ ÿßŸÜÿß ÿ®Ÿáÿß ÿ≠ÿ™Ÿä ŸäŸÉŸàŸÜ ŸÑŸä ÿßŸÑÿ≥ÿ®ŸÇ!</p>
<p>ŸàÿßŸÑŸÉÿ´Ÿäÿ± ŸÖŸÜ ŸÑÿ≠ÿ∏ÿßÿ™ ÿßŸÑÿßŸÜÿßŸÜŸäÿ© Ÿàÿ≠ÿ® ÿßŸÑÿ∞ÿßÿ™ ŸàŸÑÿ∞ÿ© ÿßŸÑÿπŸÑŸÖ ÿ™ÿ£ÿ™Ÿä ÿπŸÑŸä ÿ≠ÿ≥ÿßÿ® ÿßŸÑÿ∫ÿßŸäÿ© ÿßŸÑÿßŸàŸÑŸä Ÿà ŸÖÿ±ÿ∂ÿßÿ© ÿßŸÑŸÑŸá ÿå Ÿà ÿ≠ÿ≥ŸÜ ÿßŸÑÿπŸÑŸÖ ŸàÿßŸÑÿ•ÿÆŸÑÿßÿµ ŸÅŸä ÿßŸÑÿπŸÖŸÑ ŸàÿßŸÑŸÉÿ´Ÿäÿ± ŸÖŸÜ ÿßŸÑÿ´ŸÖÿ±ÿßÿ™ ÿßŸÑÿ™Ÿä ÿ£ÿ∂ŸäÿπŸáÿß ŸÅÿ£ÿµÿ®ÿ≠ ÿπÿ®ÿØÿß ŸÑŸÑÿπÿ®ÿßÿØÿ© (ÿ∑ŸÑÿ® ÿßŸÑÿπŸÑŸÖ ) ŸàŸÑÿ≥ÿ™ ÿπÿ®ÿØÿß ŸÑŸÑŸá.</p>
</section>
<section id="ŸáŸÑ-Ÿáÿ∞ÿß-ŸáŸà-ÿßŸÑÿ≠ŸÇ" class="level2">
<h2 class="anchored" data-anchor-id="ŸáŸÑ-Ÿáÿ∞ÿß-ŸáŸà-ÿßŸÑÿ≠ŸÇ">ŸáŸÑ Ÿáÿ∞ÿß ŸáŸà ÿßŸÑÿ≠ŸÇÿü</h2>
<p>ÿßŸÑÿ≠ŸÖÿØ ŸÑŸÑŸá ÿßŸÑÿ∞Ÿä ÿ£ÿ±ÿßŸÜŸä ÿ®ÿπÿ∂ ŸÖŸÜ ÿ£ÿØŸÜÿßÿ≥ ÿßŸÑŸÇŸÑÿ® ŸàŸÅÿ≥ÿßÿØ ÿßŸÑŸÜŸäÿ©ÿå ŸáŸÑ Ÿáÿ∞ÿß ŸáŸà ÿßŸÑÿ≠ŸÇ! ÿ®ÿßŸÑÿ™ÿßŸÉŸäÿØ ŸÑÿß.</p>
<p>ŸÖÿßŸáŸà ÿßŸÑÿ≠ŸÇ ÿ•ÿ∞ÿß‚Ä¶ŸáŸÜÿßŸÉ ÿßŸÑŸÉÿ´Ÿäÿ± ŸÖŸÜ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿßÿ™ ŸÑŸÉŸÜ ŸÖÿß Ÿäÿ≠ÿ∂ÿ± ŸÅŸä ŸÇŸÑÿ®Ÿä ÿ≠ÿßŸÑŸäÿß ÿ®ÿπÿØ ÿßŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸä ŸÉÿ´Ÿäÿ± ŸÖŸÜ ŸÖÿß ÿ£ÿ≠ÿ® ŸÖŸÜ ÿπŸÑŸÖ ŸàŸÖÿßŸÑ Ÿà ÿπŸÑÿßŸÇÿßÿ™ Ÿà ÿ∫Ÿäÿ±Ÿá..ÿ£ÿ¨ÿØ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÇÿ±ÿßŸÜŸäÿ© ŸáŸä ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑŸàÿ≠ŸäÿØŸá ÿßŸÑÿ¥ÿßŸÅŸäŸá ŸÑÿ™ÿπÿ®ÿ± ÿπŸÜ ŸÖÿß ÿ£ŸÖÿ± ÿ®Ÿá ŸÖŸÜ ÿ£ÿ≤ŸÖÿ© ŸáŸàŸäÿ© ÿ≠ŸÇŸäŸÇÿ©.</p>
<ul>
<li><strong>{ ŸÇŸèŸÑŸí ŸáŸéŸÑŸí ŸÜŸèŸÜŸéÿ®ŸêŸëÿ¶ŸèŸÉŸèŸÖŸí ÿ®ŸêÿßŸÑÿ£ŸéÿÆŸíÿ≥Ÿéÿ±ŸêŸäŸÜŸé ÿ£ŸéÿπŸíŸÖŸéÿßŸÑÿß * ÿßŸÑŸéŸëÿ∞ŸêŸäŸÜŸé ÿ∂ŸéŸÑŸéŸë ÿ≥ŸéÿπŸíŸäŸèŸáŸèŸÖŸí ŸÅŸêŸä ÿßŸÑŸíÿ≠ŸéŸäŸéÿßÿ©Ÿê ÿßŸÑÿØŸèŸëŸÜŸíŸäŸéÿß ŸàŸéŸáŸèŸÖŸí ŸäŸéÿ≠Ÿíÿ≥Ÿéÿ®ŸèŸàŸÜŸé ÿ£ŸéŸÜŸéŸëŸáŸèŸÖŸí ŸäŸèÿ≠Ÿíÿ≥ŸêŸÜŸèŸàŸÜŸé ÿµŸèŸÜŸíÿπŸãÿß }</strong> ÿ≥Ÿàÿ±ÿ© ÿßŸÑŸÉŸáŸÅ.</li>
<li>ÿ≥Ÿàÿ±ÿ© ÿßŸÑÿ≠ÿØŸäÿØ <iframe width="100%" height="300" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/301132298&amp;color=%23ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=true&amp;visual=true"></iframe>
<div style="font-size: 10px; color: #cccccc;line-break: anywhere;word-break: normal;overflow: hidden;white-space: nowrap;text-overflow: ellipsis; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif;font-weight: 100;">
<a href="https://soundcloud.com/arwasharaqi" title="ÿ£ÿ±ŸíŸàŸéŸâ." target="_blank" style="color: #cccccc; text-decoration: none;">ÿ£ÿ±ŸíŸàŸéŸâ.</a> ¬∑ <a href="https://soundcloud.com/arwasharaqi/al-7adeed" title="ÿ≥Ÿàÿ±ÿ© ÿßŸÑÿ≠ÿØŸäÿØ - ÿßŸÑÿ®ÿ±ÿßÿ° ÿ®ÿµŸÅÿ±" target="_blank" style="color: #cccccc; text-decoration: none;">ÿ≥Ÿàÿ±ÿ© ÿßŸÑÿ≠ÿØŸäÿØ - ÿßŸÑÿ®ÿ±ÿßÿ° ÿ®ÿµŸÅÿ±</a>
</div></li>
</ul>
<p>ŸàŸÑŸäÿ≥ ÿπŸÜÿØ ŸÖÿß ŸäŸÇÿßŸÑ ÿ®ÿπÿØ ÿ≥Ÿàÿ±ÿ© ÿßŸÑÿ≠ÿØŸäÿØ ŸàŸÖÿßŸÅŸäŸáÿß ŸÖŸÜ ÿÆÿ∑ÿßÿ® Ÿäÿ≠ÿ±ŸÉ ÿßŸÑÿµÿÆÿ± Ÿà Ÿäÿ¥ŸÅŸä ÿßŸÑÿµÿØÿ± Ÿà ŸäÿπŸÑŸä ÿßŸÑŸÑŸáŸÖÿ© ŸàŸäÿπÿ∑Ÿä ŸÑŸÑÿπÿ®ÿØ ÿßŸÑŸÜÿ∏ÿßÿ±ÿ© ÿßŸÑÿ≠ŸÇŸäŸÇÿ© ŸÑÿ±ÿ§Ÿäÿ© ÿßŸÑÿ≠ŸÇ.</p>
</section>
<section id="ÿ∑ŸÑÿ®-ÿßŸÑÿ™ŸàŸÅŸäŸÇ-Ÿà-ÿßŸÑÿµŸÑÿßÿ≠" class="level2">
<h2 class="anchored" data-anchor-id="ÿ∑ŸÑÿ®-ÿßŸÑÿ™ŸàŸÅŸäŸÇ-Ÿà-ÿßŸÑÿµŸÑÿßÿ≠">ÿ∑ŸÑÿ® ÿßŸÑÿ™ŸàŸÅŸäŸÇ Ÿà ÿßŸÑÿµŸÑÿßÿ≠</h2>
<p>ŸÅŸä ÿßŸÑÿ≠ŸÇŸäŸÇÿ© ŸÉŸÑ ŸÖÿß ŸÅŸä ÿßŸÑÿ£ÿπŸÑŸä ŸáŸà ÿ™ŸÖŸáŸäÿØ ÿ£ŸÜ ŸÉŸÑ ÿ£ŸÖÿßŸÑŸä Ÿà ÿ∑ŸÖŸàÿ≠ÿßÿ™Ÿä ŸáŸä ÿ±ÿ∫ÿ®ÿ© ŸÅŸä ÿßŸÑŸÖŸÜÿßŸÅÿ≥ÿ© Ÿà ÿßŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸä ÿßŸÑÿ™ŸÇÿØŸäÿ± ŸàÿßŸÑÿ±ŸÅÿπÿ© ŸÅŸä ÿßŸÑÿØŸäŸÜÿß‚Ä¶ŸàŸáÿ∞ÿß ÿÆÿ≥ÿ±ÿßŸÜ ŸÖÿ®ŸäŸÜ.</p>
<p>ÿ£ÿ±ÿ∫ÿ® ŸÅÿπŸÑÿß ÿ£ŸÜ ÿ™ŸÉŸàŸÜ ÿ™ÿ™ÿ≠ŸàŸÑ Ÿáÿ∞Ÿá ÿßŸÑÿ±ÿ∫ÿ®ÿ© ÿßŸÑŸä ÿ±ÿ∫ÿ®ÿ© ŸÅŸäŸÖÿß ÿπŸÜÿØ ÿßŸÑŸÑŸá Ÿà ŸÖÿ≠ÿ®ÿ™ÿ© Ÿà ÿ±ÿ∂ÿßŸá ŸÅŸáÿ∞ÿß ŸáŸà ÿßŸÑÿ™ŸàŸÅŸäŸÇ Ÿà ÿ£ŸÜ ÿ™ÿ±ÿ™ŸÅÿπ ÿØÿ±ÿ¨ÿßÿ™Ÿä ŸÅŸä ÿßŸÑÿØŸÜŸäÿß ÿ®ÿ£ŸÉÿ®ÿ± ŸÇÿØÿ± ŸÇÿ®ŸÑ ŸÅŸàÿßÿ™ ÿßŸÑÿ£ŸàÿßŸÜ ÿπŸÜÿØŸÖÿß ÿ™ŸÜŸÇŸÑÿ® ŸÉŸÑ ÿ£ÿπŸÖÿßŸÑŸä ÿßŸÑŸä ÿ≠ÿ≥ÿ±ÿ© Ÿà ŸÇÿ®ŸÑ ÿ£ŸÜ Ÿäÿ∂ŸÑ ÿ≥ÿπŸä Ÿà ÿ£ŸÜÿß ÿ£ÿ≠ÿ≥ÿ® ÿ£ŸÜŸä ÿßÿ≠ÿ≥ŸÜ ÿßŸÑÿπŸÖŸÑ.</p>
<p>ÿ®ÿπÿØ ŸÉŸÑ ÿßŸÑŸÖÿµÿßÿπÿ® ÿßŸÑÿ™Ÿä ÿ™ŸÇÿßÿ®ŸÑŸÜŸä ŸÖŸÜ ÿ≠ÿßÿ¨ÿ© ŸÖÿßÿØÿ© Ÿàÿ≠ÿßÿ¨ÿ© ŸÑŸÑŸÅŸáŸÖ ŸàÿßŸÑŸàŸÇÿ™ ŸàÿßŸÑÿ™ŸàŸÅŸäŸÇ ŸàÿßŸÑÿπŸÑŸÖ Ÿà ÿ∫Ÿäÿ± Ÿáÿ∞ÿß..ÿ£ÿ¨ÿØ ÿ£ŸÜ ÿßŸÑÿßŸÖÿ± ŸÉŸÑŸá ÿ®ŸäÿØ ÿßŸÑŸÑŸá Ÿà ŸÇÿØÿ±ÿ™Ÿá.</p>
<p><strong>ŸÅŸÑÿß ÿ≠ŸàŸÑ ŸàŸÑÿß ŸÇŸàÿ© ÿßŸÑÿ•ÿ®ÿßŸÑŸÑŸá Ÿà ÿ≠ÿ≥ÿ®Ÿä ÿßŸÑŸÑŸá ŸàŸÜÿπŸÖ ÿßŸÑŸàŸÉŸäŸÑ.</strong></p>
<p>Ÿäÿ¨ÿ® ÿπŸÑŸä ÿ™ŸÇÿ®ŸÑ ÿßŸÑŸÖÿ≠ÿØŸàÿØŸäÿ© Ÿà ÿßŸÑÿ∂ÿπŸÅ Ÿà ÿßŸÑŸÑÿ¨Ÿàÿ° ÿßŸÑŸä ŸÖŸÜ ÿ®ŸäÿØŸá ŸÖŸÑŸÉŸàÿ™ ÿßŸÑÿ≥ŸÖŸàÿßÿ™ Ÿà ÿßŸÑÿ£ÿ±ÿ∂ ÿßŸÑŸÇÿßÿØÿ± ÿπŸÑŸä ŸÉŸÑ ÿ¥ÿ¶. ÿ≥ÿ®ÿ≠ÿßŸÜ ÿ±ÿ®Ÿä Ÿà ÿ™ÿπÿßŸÑŸä Ÿà ÿµŸÑŸä ÿßŸÑŸÑŸá Ÿàÿ≥ŸÑŸÖ ÿπŸÑŸä ÿ≥ŸäÿØŸÜÿß ŸÖÿ≠ŸÖÿØ.</p>


</section>

 ]]></description>
  <category>blogging</category>
  <category>til</category>
  <category>ÿ™ÿ≤ŸÉŸäŸá</category>
  <guid>https://kareemai.com/til/tils/2025-05-21-til.html</guid>
  <pubDate>Tue, 20 May 2025 21:00:00 GMT</pubDate>
  <media:content url="https://kareemai.com/til/tils/images/isreal.png" medium="image" type="image/png" height="78" width="144"/>
</item>
<item>
  <title>Domains Day: Connecting Railway with Vercel, Cloudflare, and Hostinger</title>
  <dc:creator>Kareem </dc:creator>
  <link>https://kareemai.com/til/tils/2025-05-20-til.html</link>
  <description><![CDATA[ 





<section id="moving-my-domains" class="level2">
<h2 class="anchored" data-anchor-id="moving-my-domains">Moving My Domains</h2>
<p>I decided to transfer all 11 of my domains from Hostinger to other providers. I consulted two friends‚Äîa team lead in Canada and an SEO specialist in Turkey‚Äîand both recommended Porkbun.</p>
<p>I registered with Porkbun and was required to verify my identity. They offered multiple verification options, and I chose to use my government ID. However, I faced issues: I uploaded my ID about 10 times, and it failed every time, even though the verification platform supported my language. I reached out to their help center, and the fastest response came via email.</p>
<p>It took about 12 hours to receive a reply, which felt like a long time. The good news? They instantly verified my ID with no further issues. Later, when I asked another question, their response again took several hours, unlike Hostinger, which typically responds within 1 to 30 minutes (and at most within 2 hours). However, I found Hostinger‚Äôs customer service stricter and sometimes less friendly.</p>
<p>I‚Äôll discuss my reasons for moving later, but for now, let‚Äôs focus on the process.</p>
<section id="moving-from-hostinger-to-porkbun" class="level3">
<h3 class="anchored" data-anchor-id="moving-from-hostinger-to-porkbun">Moving from Hostinger to Porkbun</h3>
<p>I had never transferred a domain before, so I expected it to be challenging. However, Hostinger made the process straightforward without asking why I was transferring.</p>
<p>Here‚Äôs how to do it:</p>
<ol type="1">
<li>Go to your Hostinger dashboard.</li>
<li>Disable the <strong>Privacy Protection (WHOIS Privacy Protection)</strong> button.</li>
<li>Disable the <strong>Transfer Lock</strong> button. This may take up to 12 hours to update.</li>
<li>Obtain the <strong>Authorization Code</strong> (EPP code) from the dashboard and save it, as you‚Äôll need to provide it to Porkbun for the domain transfer.</li>
</ol>
<p>On Porkbun‚Äôs website: - Enter your domain name in the <strong>Domain Name</strong> field under the transfer section. - Copy the authorization code from Hostinger and paste it into the <strong>Auth Code</strong> field. - Click <strong>Submit</strong>.</p>
<p>The transfer(s) will be added to your cart. From there, click <strong>Continue to Billing</strong> to pay for the transfer. It‚Äôs that simple!</p>
<p>If your domain is older than 60 days, the transfer typically takes 5 to 7 days. Some domains may transfer in as little as 2 days, but Hostinger will send a verification request via email to confirm the transfer.</p>
<p>Note: If your domain is less than 60 days old, you‚Äôll need to wait until it passes the 60-day mark. There‚Äôs talk of this being reduced to 30 days, but as of this writing (May 2025), the 60-day rule applies, per ICANN regulations, not Hostinger or Porkbun.</p>
</section>
<section id="connecting-railway-and-vercel-with-porkbun" class="level3">
<h3 class="anchored" data-anchor-id="connecting-railway-and-vercel-with-porkbun">Connecting Railway and Vercel with Porkbun</h3>
<p>Connecting a domain to Vercel is straightforward. Follow the instructions provided by Vercel, which are similar to those for any registrar. Add the necessary DNS records in Porkbun‚Äôs <strong>DNS Management</strong> section, accessible from the domain management dashboard under your account menu.</p>
<p>Connecting to Railway was trickier. I needed an intermediary, and I chose Cloudflare to manage my Porkbun domain‚Äôs DNS. Here‚Äôs how it worked:</p>
<ol type="1">
<li><p>Configure Cloudflare to manage your Porkbun domain.</p></li>
<li><p>Add the necessary Railway DNS records to Cloudflare.</p></li>
<li><p>Wait approximately 1 to 2 days for the DNS changes to propagate.</p></li>
</ol>
<p>Once the DNS updates are complete, your domain should work seamlessly with Railway.</p>


</section>
</section>

 ]]></description>
  <category>blogging</category>
  <category>til</category>
  <category>seo</category>
  <category>domains</category>
  <category>vercel</category>
  <category>porkbun</category>
  <category>web</category>
  <guid>https://kareemai.com/til/tils/2025-05-20-til.html</guid>
  <pubDate>Mon, 19 May 2025 21:00:00 GMT</pubDate>
  <media:content url="https://kareemai.com/til/tils/til.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Starting Substack as a AI Researcher</title>
  <dc:creator>kareem </dc:creator>
  <link>https://kareemai.com/til/tils/2025-05-19-til.html</link>
  <description><![CDATA[ 





<section id="what-is-substack" class="level2">
<h2 class="anchored" data-anchor-id="what-is-substack">What is Substack?</h2>
<p>From their about page: ‚ÄúOn Substack, writers and creators can publish their work and make money from paid subscriptions while supporters can directly sustain the work they deeply value.‚Äù</p>
<p>It‚Äôs a platform where you can publish what you want in an organized way, with great analytics tools and a powerful recommendation and search system.</p>
<p>I like to think of it as a nicer version of Medium, with more than just SEO dumped from Google.</p>
<p>I really love the network from a design perspective. It‚Äôs an organized recommendation system based on multiple aspects.</p>
</section>
<section id="why-use-substack-as-an-ai-researcher" class="level2">
<h2 class="anchored" data-anchor-id="why-use-substack-as-an-ai-researcher">Why use Substack as an AI Researcher?</h2>
<p>I used LinkedIn and Medium, and I don‚Äôt find them very useful for engagement or building a real network.</p>
<p>My gravity is very tiny in these networks and I can‚Äôt get bigger for multiple reasons I will discuss later.</p>
<p>The same is true for X, but it‚Äôs a very good place for the ML community.</p>
<p>My reasons:</p>
<ol type="1">
<li><p>Improve my writing style. I am not good at writing or English, but I am trying to do my best. Communication is a very important skill for my work as a researcher.</p></li>
<li><p>I build products I want people to learn more about‚Ä¶more later.</p></li>
<li><p>I have services I want to market‚Ä¶more later.</p></li>
</ol>
<p>Did you forget something?</p>
<p>^_^</p>
<p>Die, empty, and publish your knowledge!</p>
<p>Actually, the main reason for me to write is that I feel lonely in my journey of learning about DL and software engineering.</p>
<p>And I feel bored most of the time.</p>
<p>I love the feeling that I am writing and archiving my life. This reminds me that I am doing hard work, learning new things, and my life will not end without doing great things from my point of view.</p>
<p>Recapping what I learned when I write about it makes the concepts stick in your mind, and you get more insights from people‚Äôs comments. This is very helpful for me.</p>
<p>For readers, you can see what I do, and you may be interested in it, be inspired, and learn from my mistakes.</p>
<section id="following-the-giants" class="level3">
<h3 class="anchored" data-anchor-id="following-the-giants">Following the Giants?</h3>
<p>When I started learning about DL, I searched for a great book about DL and PyTorch that is well explained in both code and theory.</p>
<p>I found his Substack where he publishes what he learns. At the same time, I found similar people like</p>
<ul>
<li><p><a href="https://substack.com/@jayalammar">Jay Alammar</a></p></li>
<li><p><a href="https://newsletter.maartengrootendorst.com/">Maarten Grootendorst</a></p></li>
<li><p><a href="https://decodingml.substack.com/p/llm-engineers-handbook-is-finally">Paul Iusztin</a></p></li>
</ul>
<p>These people I follow and appreciate what they provide!</p>
<p>So I started to ask, why do they use Substack?</p>
<p>The answer is very clear if you know them ^_^</p>
</section>
<section id="quick-analysis" class="level3">
<h3 class="anchored" data-anchor-id="quick-analysis">Quick analysis</h3>
<p>The UI/UX, speed of the website, and animations are very cool.</p>
<p>Let‚Äôs look at the level I want to reach :)</p>
<section id="ahead-of-ai" class="level4">
<h4 class="anchored" data-anchor-id="ahead-of-ai">Ahead of AI</h4>
<p>On 28/04/2023</p>
<p>It reached 15,028 free subscribers and only 69 paid ones!</p>
<p>This is very disappointing for me.</p>
<p>You don‚Äôt know if these are monthly or yearly paid and how much they give.</p>
<p>Of course, I don‚Äôt know how much he earns or others. I want to motivate myself and not create high expectations. He is doing great stuff, really great. His content is in the top 5 for me for AI and real content.</p>
<p>No scams, no ‚Äúread this paper,‚Äù ‚Äúlook at this book,‚Äù ‚Äúhere are the top 1000 chatbots that are better than 10 who are better than GPT!!‚Äù</p>
<p>Oh my god, QwenClaudeMixtral just released a model that will convert space into water in the year 1021932103120931920.</p>
<p>This is very silly if you respect your readers‚Äô minds!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://kareemai.com/til/tils/images/old_ahead_of_ai.jpg" class="img-fluid figure-img"></p>
<figcaption>Ahead of AI subscribers in 2024</figcaption>
</figure>
</div>
<p>What about now?</p>
<p>He has more than 105k+ and is #71 Rising in Technology. I don‚Äôt know why 71!! He must be top 5.</p>
<p><strong>Jay Alammar</strong> =&gt; 23k+ subscribers</p>
<p><strong>Maarten Grootendorst</strong> =&gt; 20k+ subscribers<br>
<strong>Paul Iusztin</strong> =&gt; 25k+ subscribers</p>
<p>Most likes per post are around 10 to 20 only!</p>
<p>The comments are around 0 to 5 :)</p>
</section>
</section>
<section id="my-goal-in-the-coming-6-months" class="level3">
<h3 class="anchored" data-anchor-id="my-goal-in-the-coming-6-months">My Goal in the coming 6 months?</h3>
<p>I am doing multiple things at the current time.</p>
<p>I am on 4 projects and doing my Master Thesis and a lot of projects and websites I am developing.</p>
<p>I will not be able to provide much, but I will try my best.</p>
<p>I don‚Äôt care about the number of subscribers, I care about real ones, who will comment and read the content. If I can get 30 people to read it, this is great. Really, if you imagine 30 people in your room watching what you did, it‚Äôs mind-blowing. Imagine 300 or 3,000‚Ä¶Wow.</p>
<p>For money, if I can get only $200 per month, this is very great as a start.</p>
<section id="disease-from-other-websites" class="level4">
<h4 class="anchored" data-anchor-id="disease-from-other-websites">Disease from other websites?</h4>
<p>I noticed the following issues on the first day:</p>
<ol type="1">
<li><p>Sh*** shorts and copyright issues: By copyright, I mean some people take others‚Äô content from outside Substack and post it as if it is their own and get followers for this! This is very bad. I don‚Äôt like strict copyright like ‚Äúyou copied this sentence‚Äù or ‚Äúused a logo,‚Äù but stealing the whole content!!!</p></li>
<li><p>Very short articles compared to normal tweets! I found a lot of articles that appeared to me are like tweets‚Ä¶I thought I was in a place where people write in-depth content, not clickbait.</p></li>
</ol>
<p>I don‚Äôt talk about the post but the long-form ones.</p>
<ol start="3" type="1">
<li>Drop your Substack fake numbers: A lot of people post ‚Äúdrop your Substack below and whatever it‚Äôs about I will exchange‚Äù and multiple similar things!! What is the</li>
<li>purpose then‚Ä¶ you have 1 million subscribers, then what?</li>
</ol>
</section>
<section id="there-is-no-official-api-for-substack" class="level4">
<h4 class="anchored" data-anchor-id="there-is-no-official-api-for-substack">There is no official API for Substack</h4>
<p>The most annoying thing in Substack is there is no current API to develop apps, create extensions, and automate a lot of things.</p>
</section>
</section>
<section id="what-things-can-i-write-about" class="level3">
<h3 class="anchored" data-anchor-id="what-things-can-i-write-about">What things can I write about?</h3>
<p>You seem to love to talk a lot? What will you write then?</p>
<p>It‚Äôs very obvious‚Ä¶</p>
<p>I want to talk about the following:</p>
<ol type="1">
<li><p>Compute world with AI, fine-tuning/training, and cloud instances, etc.</p></li>
<li><p>Products I use and tried, not product reviews.</p></li>
<li><p>Books and papers I read and summarized.</p></li>
<li><p>TILs, what I learned today!</p></li>
<li><p>Novel writing about the AI world like vector databases, embedding, and searching, etc.</p></li>
<li><p>Applications I build and services I provide.</p></li>
<li><p>My workflow and software i use.</p></li>
<li><p>Substack tips and analysis.</p></li>
</ol>
<p>you can find links here for now: 1. Gpuvec publications</p>
<iframe src="https://gpuvecc.substack.com/embed" width="480" height="320" style="border:1px solid #EEE; background:white;" frameborder="0" scrolling="no">
</iframe>
<ol start="2" type="1">
<li>Kareem‚Äôs TILs</li>
</ol>
<iframe src="https://kareemnns.substack.com/embed" width="480" height="320" style="border:1px solid #EEE; background:white;" frameborder="0" scrolling="no">
</iframe>


</section>
</section>

 ]]></description>
  <category>blogging</category>
  <category>til</category>
  <category>seo</category>
  <category>substack</category>
  <guid>https://kareemai.com/til/tils/2025-05-19-til.html</guid>
  <pubDate>Sun, 18 May 2025 21:00:00 GMT</pubDate>
  <media:content url="https://kareemai.com/til/tils/images/old_ahead_of_ai.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Explore the Qdrant Blog core concepts | Part 1</title>
  <dc:creator>kareem </dc:creator>
  <link>https://kareemai.com/til/tils/2025-05-18-til.html</link>
  <description><![CDATA[ 





<section id="overview-of-qdrant-features-and-concepts" class="level2">
<h2 class="anchored" data-anchor-id="overview-of-qdrant-features-and-concepts">Overview of Qdrant features and Concepts</h2>
<p>I will divide the blogs into 3 types:</p>
<ol type="1">
<li><p>Startups using Qdrant: why and comparision you get read nice usecases with some numbers of comparision and how qdrant is amazing</p></li>
<li><p>Startup using Qdrant++: The same but with code snippets and System Design discussion..very useful for me as a developer</p></li>
<li><p>Qdrant releases: New features in Qdrant and how to use them.</p></li>
</ol>
<p><strong>Remember this when i will build Agentic RAG in the new job</strong></p>
<p>users tend to ask more structured, analytical questions when they know a database is involved‚Äîqueries better suited to SQL than vector search. This prompted the team to pair Qdrant with a text-to-SQL system, blending unstructured and structured query capabilities for a more versatile agent.</p>
<section id="hotel-search-with-vectors" class="level3">
<h3 class="anchored" data-anchor-id="hotel-search-with-vectors">Hotel Search with Vectors</h3>
<p>Superlinked enhances search by embedding each attribute (text, numbers, categories) into specialized spaces, enabling nuanced, multi-attribute queries.</p>
<p>An LLM interprets user intent, assigning weights to preferences (e.g., price, rating), allowing flexible, business-driven ranking without system redesign.</p>
<p>Hard filters narrow results, while weighted nearest neighbor search ranks them by user preferences.</p>
<p>This unified approach supports multimodal search‚Äîcombining semantic text, scaled numerical, and categorical data‚Äîpreserving relationships and preference strengths.</p>
<p>Unlike traditional systems that separate or flatten data, Superlinked enables simultaneous, weighted consideration of all attributes, solving challenges like reconciling</p>
<p>results across types and capturing nuanced user intent.</p>
</section>
<section id="reciprocal-rank-rusion-rrf" class="level3">
<h3 class="anchored" data-anchor-id="reciprocal-rank-rusion-rrf">Reciprocal Rank Rusion (RRF)</h3>
<p>Qdrant‚Äôs native support for Reciprocal Rank Fusion (RRF) streamlined their retriever implementations, reducing hybrid search code by 80%. The multi-vector capabilities also enabled more sophisticated retrieval methods that better captured semantic relationships.</p>
</section>
<section id="qdrant-1.13-gpu-indexing" class="level3">
<h3 class="anchored" data-anchor-id="qdrant-1.13-gpu-indexing">Qdrant 1.13 GPU Indexing</h3>
<p>Here is summary of these new features</p>
<section id="gpu-accelerated-indexing-with-qdrant" class="level4">
<h4 class="anchored" data-anchor-id="gpu-accelerated-indexing-with-qdrant">GPU Accelerated Indexing with Qdrant</h4>
<p><strong>You can Index over all majro GPU vendors including NVIDIA,AMD and Intel that support Vulkan API to get speeds up to 10x faster than CPU-based methods</strong>*</p>
<p>As of right now this solution supports only on-premises deployments, but they will introduce support for Qdrant Cloud shortly.</p>
<p>Additional benefits:</p>
<ol type="1">
<li><p>Multi-GPU support</p></li>
<li><p>GPU indexing supports all quantization options and datatypes in Qdrant</p></li>
</ol>
</section>
<section id="strict-mode-for-opertional-control" class="level4">
<h4 class="anchored" data-anchor-id="strict-mode-for-opertional-control">Strict mode for Opertional Control</h4>
<p>Strict Mode enforces operational controls in distributed Qdrant deployments. It limits resource-intensive operations (like unindexed filtering and large batch sizes), sets boundaries on search parameters, and adds safeguards for payload sizes and timeouts. This prevents system overload, solves the ‚Äúnoisy neighbor‚Äù problem, and ensures reliable performance‚Äîespecially in multi-tenant or serverless environments.</p>
</section>
<section id="hnsw-graph-compression" class="level4">
<h4 class="anchored" data-anchor-id="hnsw-graph-compression">HNSW Graph Compression</h4>
<p>Make search lighter on memory wihtout sacrificing speed with Delta Encoding.</p>
<p>Delta Encoding is a clever way to compress data by storing only the differences (or ‚Äúdeltas‚Äù) between values. It‚Äôs commonly used in search engines (for the classical inverted index) to save space and improve performance. <em>I think i have read this with Colbertv2 using similar techniques to reduce the siz </em> it‚Äôs called <strong>residual compression mechanism</strong> needs more searching</p>
<p>It‚Äôs now used for HNSW graph structure that powers Qdrant‚Äôs search.</p>
</section>
</section>
</section>
<section id="static-embedding-with-qdrant-and-model2vec" class="level2">
<h2 class="anchored" data-anchor-id="static-embedding-with-qdrant-and-model2vec">Static Embedding with Qdrant and Model2vec</h2>
<p>Static embedding from minishLab reduce the model size with 15x reduction and up to 500x speed increase while the maintain more than 85% of the performance levels. it‚Äôs like our <a href="https://kareemai.com/blog/posts/minishlab/zaraah.html">zaraah model</a> for arabic.</p>
<p>Static embedding are dense embedding so you can also use with qdrant collections. The retrieval is not going to be any faster becuase static embeddings. but the speedup is in creating the vectors from your data and encoding the queries.</p>
<p>If you want to make the retrieval faster use the following: 1. Matryoshka Embeddings 2. Quantization methods like (Scalar and Binary Quantization) ### When to use Static Embeddings ?</p>
<ol type="1">
<li><p><strong>Mobile applications</strong> - although many smartphones have powerful CPUs or even GPUs, the battery life is still a concern, and the static embeddings might be a good compromise between the quality and the power consumption. Moreover, the static embeddings can be used in the applications that require offline mode.</p></li>
<li><p><strong>Web browser extensions</strong> - running a transformer-based model in a web browser is usually not quite an option, but static embeddings might be a good choice, as they have fewer parameters and are faster to encode.</p></li>
<li><p><strong>Embedded systems</strong> - the static embeddings might be a good choice for the devices with limited computational power, such as IoT devices or microcontrollers.</p></li>
</ol>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References:</h3>
<p>There is more text in here from Qdrant not me..you can continue reading here</p>
<ol type="1">
<li><p><a href="https://qdrant.tech/blog/superlinked-multimodal-search/">Hotel Search</a></p></li>
<li><p><a href="https://qdrant.tech/blog/">Qdrant Blog</a></p></li>
<li><p><a href="https://qdrant.tech/blog/static-embeddings/">Static Embedding</a></p></li>
</ol>


</section>
</section>

 ]]></description>
  <category>blogging</category>
  <category>til</category>
  <category>qdrant</category>
  <guid>https://kareemai.com/til/tils/2025-05-18-til.html</guid>
  <pubDate>Sat, 17 May 2025 21:00:00 GMT</pubDate>
  <media:content url="https://kareemai.com/til/tils/til.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>TIL ? Tody I lernt to create TIL</title>
  <dc:creator>kareem </dc:creator>
  <link>https://kareemai.com/til/tils/2025-05-17-til.html</link>
  <description><![CDATA[ 





<section id="why-til" class="level2">
<h2 class="anchored" data-anchor-id="why-til">Why TIL ?</h2>
<p>It helps you overcome the perfacationsim in writting. you don‚Äôt need to create great article in depth about things you want to share or learn about..etc, all what you want it to write a thing you were trying to learn or solve today and how you solve it</p>
<p>which will help me more focused and make the process of learning more easier and useful for me and others</p>
</section>
<section id="create-your-own-gravitey" class="level2">
<h2 class="anchored" data-anchor-id="create-your-own-gravitey">create your own gravitey</h2>
<p>sometimes you want to reach and communicate with people in the same space of problems you are solving, search engines are very bad in provide the information you want..this is related to how these engines work and other SEO stuff. but for me, i can‚Äôt reach people and help them or get benefit from them because they simple don‚Äôt know about me!!</p>
<p>TIL will decreases this spaces and daily TIl about the things i am learning which are alot will start to give me nice SEO and unique because i am talking about things i don‚Äôt know and i am interested in and there will be much people in the same boat this will increase my X account and linkedin and this is very useful in the current time.</p>
</section>
<section id="tils-level-up" class="level2">
<h2 class="anchored" data-anchor-id="tils-level-up">TILs level up</h2>
<p>I also want to think in way to extract more crafted blogs from my TILs. but just start making it habbit and will see who will it comes in the end.</p>
<p>initial thoughts: 1. weekly recap from my TILs and SEO optimization for the keywords that is increasing and i am interested in 2. ML to extract related stuff, for example i will want to take about the folloinwg topics: - Late interaciton (Pylate, Colbertv2, ColPali) - Searching - Model2vec - Visino Language models..etc collecting them and start writing about them and adding internal links for them will be very useful to improve my blog system</p>
</section>
<section id="things-i-learned-the-last-2-days" class="level2">
<h2 class="anchored" data-anchor-id="things-i-learned-the-last-2-days">Things i learned the last 2 days</h2>
<section id="the-best-way-to-increase-traffic-is-comment-with-valuble-knowledge." class="level3">
<h3 class="anchored" data-anchor-id="the-best-way-to-increase-traffic-is-comment-with-valuble-knowledge.">The best way to increase traffic is comment with valuble knowledge.</h3>
<p>I was scorlling on X and found some popular account tweets about Harvard CS197 AI Research and i had create a review year ago about it I add the link in the comment and just slept. Boom i found reply analytics links opened wihtout annoying anyone!! - 501 Impressions - 77 Engagements - 2 profile visits - 74 clicks</p>
</section>
</section>
<section id="what-is-palid-index" class="level2">
<h2 class="anchored" data-anchor-id="what-is-palid-index">What is PALID index?</h2>
<p>PLAID index is for indexing very large datasets with Later interaction models like (Colbertv2 &amp; ColPali) IT Solves the storage footprint and allow you to scale to infinity</p>
<p>They swapped the faiss from facebook what is nice thing because i have multiple bad time to install it especially the GPU version.</p>
<p>and used fastkeamns from the amazing <span class="citation" data-cites="bclaive">@bclaive</span> which i really like his work on embeddings</p>
<p>It‚Äôs replacement for Voyager-based HNSW index which was very bad for scaling Late-Interaction retrieval models</p>
</section>
<section id="prime-intellect-vs-gpuvec" class="level2">
<h2 class="anchored" data-anchor-id="prime-intellect-vs-gpuvec">Prime Intellect VS Gpuvec</h2>
<p>I was creating a website to find and compare the cloud compute instances from all cloud providers in easy and interactive way.</p>
<p>I started to work on it the last month, but stoped for other work.</p>
<p>suddenly i found this website which is called Prime intellect.</p>
<p>And i just want to say,woooooow. it‚Äôs a piece of art. The design and information on it and how fast, accurate is very embrassing for my poor gpuvec.com</p>
<p>should i continue improve my website?</p>
<p>Actually yes, we share similar goals but there is multiple chances to compete or even collaborite!! who knows!</p>
<p>They are more than just listing and compare prices, then enable you to use these GPUs from their website.</p>
<p>Also they create decentralized models and have a mutliple expirened engineers.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ol type="1">
<li><a href="https://x.com/antoine_chaffin/status/1923378986219958526">Antoine tweet</a></li>
<li><a href="https://betatim.github.io/posts/til-explained/">betatim TIL</a></li>
<li><a href="https://kareemai.com/til/">other TILs</a></li>
</ol>


</section>

 ]]></description>
  <category>blogging</category>
  <category>til</category>
  <guid>https://kareemai.com/til/tils/2025-05-17-til.html</guid>
  <pubDate>Fri, 16 May 2025 21:00:00 GMT</pubDate>
  <media:content url="https://kareemai.com/til/tils/til.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
