{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cff8a5f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"BM25 part 3 | Naive Benchmark\"\n",
    "description: \"BM25 Benchmarked with bm25-sparse , rankbm_25 , numpy vectorization, scipy and normal python code.\"\n",
    "author: \"kareem\"\n",
    "date: 2025-12-20\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "    code-tools: true\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "categories:\n",
    "    - blogging\n",
    "    - embedding\n",
    "    - qdrant \n",
    "image: \"images/bm25.png\"\n",
    "open-graph:\n",
    "  image: \"images/bm25.png\"\n",
    "twitter-card:\n",
    "  image: \"images/bm25.png\"\n",
    "draft: false\n",
    "execute: \n",
    "    echo: true\n",
    "jupyter: python3\n",
    "---\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import log\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {},
   "source": [
    "### Dataset Benchmark\n",
    "\n",
    "I will continue exploring the bm25 and how to use with a huggingface dataset with multiple implementations and compare their speed \n",
    "\n",
    "The methods i will use: \n",
    "\n",
    "1. Normal Python Calculations\n",
    "2. Spicy sparse matrix with python loops\n",
    "3. Spicy sparse matrix with Numpy Vectorization\n",
    "4. Rank_bm25\n",
    "5. bm25-sparse\n",
    "6. Polars optimization\n",
    "\n",
    "If there is any mistake please tell me!\n",
    "\n",
    "it's first time for me to create a benchmark for anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bkHC",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 8530 reviews\n",
      "Generated 1000 test queries\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
    "corpus = [item[\"text\"] for item in dataset]\n",
    "print(f\"Corpus size: {len(corpus)} reviews\")\n",
    "\n",
    "\n",
    "def generate_random_queries(corpus, num_queries=1000):\n",
    "    queries = []\n",
    "    for _ in range(num_queries):\n",
    "        doc = random.choice(corpus)\n",
    "        words = tokenize(doc)\n",
    "        num_words = random.randint(2, 4)\n",
    "        if len(words) >= num_words:\n",
    "            query_words = random.sample(words, num_words)\n",
    "        else:\n",
    "            query_words = words\n",
    "        queries.append(\" \".join(query_words))\n",
    "    return queries\n",
    "\n",
    "\n",
    "test_queries = generate_random_queries(corpus, 1000)\n",
    "print(f\"Generated {len(test_queries)} test queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lEQa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1. NORMAL PYTHON (LOOPS) ===\n",
      "Indexing: 0.0801s\n",
      "Query time: 23.6460s\n",
      "QPS: 42.29\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.08005261421203613, 23.645965099334717, 42.29051323551751)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def benchmark_python_loops():\n",
    "    print(\"=== 1. NORMAL PYTHON (LOOPS) ===\")\n",
    "    start = time.time()\n",
    "\n",
    "    # Tokenize all documents\n",
    "    all_docs_terms_py = []\n",
    "    for doc in corpus:\n",
    "        tokens = tokenize(doc)\n",
    "        all_docs_terms_py.append(Counter(tokens))\n",
    "\n",
    "    # Compute DF\n",
    "    df_py = {}\n",
    "    for doc_tf in all_docs_terms_py:\n",
    "        for term in doc_tf.keys():\n",
    "            df_py[term] = df_py.get(term, 0) + 1\n",
    "\n",
    "    # Average doc length\n",
    "    total_len = sum(sum(doc_tf.values()) for doc_tf in all_docs_terms_py)\n",
    "    avg_doc_len_py = total_len / len(corpus)\n",
    "\n",
    "    # IDF function\n",
    "    def compute_idf_py(term, df_dict, N):\n",
    "        if term not in df_dict:\n",
    "            return 0\n",
    "        return np.log((N - df_dict[term] + 0.5) / (df_dict[term] + 0.5))\n",
    "\n",
    "    # BM25 function\n",
    "    def bm25_score_py(term, doc_tf, idf_val, doc_len, avg_len, k1=1.5, b=0.75):\n",
    "        tf = doc_tf.get(term, 0)\n",
    "        if tf == 0:\n",
    "            return 0\n",
    "        return idf_val * tf * (k1 + 1) / (tf + k1 * (1 - b + b * (doc_len / avg_len)))\n",
    "\n",
    "    index_time = time.time() - start\n",
    "\n",
    "    # Query\n",
    "    start_query = time.time()\n",
    "    for query in test_queries:\n",
    "        query_tokens = tokenize(query)\n",
    "        doc_scores = [0] * len(corpus)\n",
    "        for token in query_tokens:\n",
    "            if token in df_py:\n",
    "                idf_val = compute_idf_py(token, df_py, len(corpus))\n",
    "                for doc_idx, doc_tf in enumerate(all_docs_terms_py):\n",
    "                    doc_len = sum(doc_tf.values())\n",
    "                    doc_scores[doc_idx] += bm25_score_py(\n",
    "                        token, doc_tf, idf_val, doc_len, avg_doc_len_py\n",
    "                    )\n",
    "        top_k = sorted(\n",
    "            range(len(doc_scores)), key=lambda i: doc_scores[i], reverse=True\n",
    "        )[:10]\n",
    "\n",
    "    query_time = time.time() - start_query\n",
    "    qps = len(test_queries) / query_time\n",
    "\n",
    "    print(f\"Indexing: {index_time:.4f}s\")\n",
    "    print(f\"Query time: {query_time:.4f}s\")\n",
    "    print(f\"QPS: {qps:.2f}\\n\")\n",
    "\n",
    "    return index_time, query_time, qps\n",
    "\n",
    "\n",
    "benchmark_python_loops()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {},
   "source": [
    "## Why Sparse Matrices?\n",
    "\n",
    "Right now, to search for \"python programming\", you'd need to:\n",
    "1. Loop through each document\n",
    "2. Calculate BM25 score for \"python\"\n",
    "3. Calculate BM25 score for \"programming\"\n",
    "4. Add them up\n",
    "\n",
    "That's slow for large datasets!\n",
    "\n",
    "**The key insight:** We can pre-compute ALL BM25 scores for ALL words in ALL documents and store them in a matrix. Then searching becomes just looking up rows and adding them.\n",
    "\n",
    "Here's what the matrix looks like conceptually:\n",
    "\n",
    "```\n",
    "               Doc0   Doc1   Doc2\n",
    "python         -0.51  -0.56   0.00\n",
    "programming    -1.95  -2.14  -1.79\n",
    "java            0.00   0.00   2.20\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Xref",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 2. SPARSE MATRIX WITH PYTHON LOOPS ===\n",
      "Indexing: 77.9146s\n",
      "Query time: 0.4709s\n",
      "QPS: 2123.64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(77.91458225250244, 0.4708900451660156, 2123.6380133019015)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def benchmark_sparse_python_loops():\n",
    "    print(\"=== 2. SPARSE MATRIX WITH PYTHON LOOPS ===\")\n",
    "    start = time.time()\n",
    "\n",
    "    # Tokenize\n",
    "    all_docs_terms_sp = []\n",
    "    for doc in corpus:\n",
    "        tokens = tokenize(doc)\n",
    "        all_docs_terms_sp.append(Counter(tokens))\n",
    "\n",
    "    # Get all words and create word_to_idx\n",
    "    all_words_sp = list(set(word for doc in all_docs_terms_sp for word in doc))\n",
    "    word_to_idx_sp = {word: idx for idx, word in enumerate(all_words_sp)}\n",
    "\n",
    "    # Compute DF\n",
    "    df_sp = {}\n",
    "    for doc_tf in all_docs_terms_sp:\n",
    "        for term in doc_tf.keys():\n",
    "            df_sp[term] = df_sp.get(term, 0) + 1\n",
    "\n",
    "    # Average doc length\n",
    "    avg_doc_len_sp = sum(sum(doc_tf.values()) for doc_tf in all_docs_terms_sp) / len(\n",
    "        corpus\n",
    "    )\n",
    "\n",
    "    # Build sparse matrix\n",
    "    rows_idx = []\n",
    "    cols_idx = []\n",
    "    data = []\n",
    "\n",
    "    for word_idx, word in enumerate(all_words_sp):\n",
    "        idf_val = np.log((len(corpus) - df_sp[word] + 0.5) / (df_sp[word] + 0.5))\n",
    "        for doc_idx, doc_tf in enumerate(all_docs_terms_sp):\n",
    "            doc_len = sum(doc_tf.values())\n",
    "            tf = doc_tf.get(word, 0)\n",
    "            if tf != 0:\n",
    "                score = (\n",
    "                    idf_val\n",
    "                    * tf\n",
    "                    * 2.5\n",
    "                    / (tf + 1.5 * (1 - 0.75 + 0.75 * (doc_len / avg_doc_len_sp)))\n",
    "                )\n",
    "                rows_idx.append(word_idx)\n",
    "                cols_idx.append(doc_idx)\n",
    "                data.append(score)\n",
    "\n",
    "    sparse_matrix_sp = csr_matrix(\n",
    "        (data, (rows_idx, cols_idx)), shape=(len(all_words_sp), len(corpus))\n",
    "    )\n",
    "\n",
    "    index_time = time.time() - start\n",
    "\n",
    "    # Query\n",
    "    start_query = time.time()\n",
    "    for query in test_queries:\n",
    "        query_indices = [\n",
    "            word_to_idx_sp[word] for word in tokenize(query) if word in word_to_idx_sp\n",
    "        ]\n",
    "        if query_indices:\n",
    "            doc_scores = np.array(\n",
    "                sparse_matrix_sp[query_indices, :].sum(axis=0)\n",
    "            ).flatten()\n",
    "            top_k = np.argsort(doc_scores)[-10:][::-1]\n",
    "\n",
    "    query_time = time.time() - start_query\n",
    "    qps = len(test_queries) / query_time\n",
    "\n",
    "    print(f\"Indexing: {index_time:.4f}s\")\n",
    "    print(f\"Query time: {query_time:.4f}s\")\n",
    "    print(f\"QPS: {qps:.2f}\\n\")\n",
    "\n",
    "    return index_time, query_time, qps\n",
    "\n",
    "\n",
    "benchmark_sparse_python_loops()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {},
   "source": [
    "## Performance Optimization with Numpy\n",
    "\n",
    "My code has python loops everywhere.\n",
    "Python loops are very slow.\n",
    "The goal here it to use Numpy Vectorization: doing operations on entiry arrays at once instead of looping :\n",
    "```\n",
    "for word_1 in all_words:\n",
    "for doc_idx, doc_1 in enumerate(all_docs_terms):\n",
    "    idf_python_1 = compute_idf(word_1, 3, df)\n",
    "    doc_len_1 = sum(all_docs_terms[doc_idx].values())\n",
    "    score_1 = bm25_score(word_1, doc_1, idf_python_1, doc_len_1, avg_doc_len)\n",
    "```\n",
    "This has nested loops => very slow for large datasets.\n",
    "Key Insight: Instead of computing one BM25 score at a time, we can compute all scores at once using matrix opertions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "BYtC",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3. SPARSE MATRIX WITH NUMPY VECTORIZATION ===\n",
      "Indexing: 6.9885s\n",
      "Query time: 0.5250s\n",
      "QPS: 1904.79\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6.988537311553955, 0.5249927043914795, 1904.7883744577036)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def benchmark_sparse_numpy():\n",
    "    print(\"=== 3. SPARSE MATRIX WITH NUMPY VECTORIZATION ===\")\n",
    "    start = time.time()\n",
    "\n",
    "    # Tokenize\n",
    "    all_docs_terms_np = []\n",
    "    for doc in corpus:\n",
    "        tokens = tokenize(doc)\n",
    "        all_docs_terms_np.append(Counter(tokens))\n",
    "\n",
    "    all_words_np = list(set(word for doc in all_docs_terms_np for word in doc))\n",
    "    word_to_idx_np = {word: idx for idx, word in enumerate(all_words_np)}\n",
    "\n",
    "    # Build TF matrix\n",
    "    rows = []\n",
    "    cols = []\n",
    "    tf_data = []\n",
    "\n",
    "    for doc_idx, doc_tf in enumerate(all_docs_terms_np):\n",
    "        for word, tf in doc_tf.items():\n",
    "            rows.append(word_to_idx_np[word])\n",
    "            cols.append(doc_idx)\n",
    "            tf_data.append(tf)\n",
    "\n",
    "    tf_matrix_np = csr_matrix(\n",
    "        (tf_data, (rows, cols)), shape=(len(all_words_np), len(corpus))\n",
    "    )\n",
    "\n",
    "    # Compute DF array\n",
    "    df_array_np = np.array((tf_matrix_np > 0).sum(axis=1)).flatten()\n",
    "\n",
    "    # Document lengths\n",
    "    doc_lengths_np = np.array(tf_matrix_np.sum(axis=0)).flatten()\n",
    "    avg_doc_len_np = doc_lengths_np.mean()\n",
    "\n",
    "    # Compute IDF array\n",
    "    N = len(corpus)\n",
    "    idf_array_np = np.log((N - df_array_np + 0.5) / (df_array_np + 0.5))\n",
    "\n",
    "    # Vectorized BM25\n",
    "    k1 = 1.5\n",
    "    b = 0.75\n",
    "    tf_dense = tf_matrix_np.toarray()\n",
    "\n",
    "    numerator = tf_dense * (k1 + 1)\n",
    "    length_norm = 1 - b + b * (doc_lengths_np / avg_doc_len_np)\n",
    "    denominator = tf_dense + k1 * length_norm\n",
    "\n",
    "    idf_column = idf_array_np.reshape(-1, 1)\n",
    "    bm25_scores = idf_column * (numerator / (denominator + 1e-10))\n",
    "\n",
    "    bm25_matrix_np = csr_matrix(bm25_scores)\n",
    "\n",
    "    index_time = time.time() - start\n",
    "\n",
    "    # Query\n",
    "    start_query = time.time()\n",
    "    for query in test_queries:\n",
    "        query_indices = [\n",
    "            word_to_idx_np[word] for word in tokenize(query) if word in word_to_idx_np\n",
    "        ]\n",
    "        if query_indices:\n",
    "            doc_scores = np.array(\n",
    "                bm25_matrix_np[query_indices, :].sum(axis=0)\n",
    "            ).flatten()\n",
    "            top_k = np.argsort(doc_scores)[-10:][::-1]\n",
    "\n",
    "    query_time = time.time() - start_query\n",
    "    qps = len(test_queries) / query_time\n",
    "\n",
    "    print(f\"Indexing: {index_time:.4f}s\")\n",
    "    print(f\"Query time: {query_time:.4f}s\")\n",
    "    print(f\"QPS: {qps:.2f}\\n\")\n",
    "\n",
    "    return index_time, query_time, qps\n",
    "\n",
    "\n",
    "benchmark_sparse_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "RGSE",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 4. RANK-BM25 ===\n",
      "Indexing: 0.0813s\n",
      "Query time: 5.0695s\n",
      "QPS: 197.26\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.08132147789001465, 5.069519281387329, 197.2573619892297)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def benchmark_rank_bm25():\n",
    "    from rank_bm25 import BM25Okapi\n",
    "\n",
    "    print(\"=== 4. RANK-BM25 ===\")\n",
    "\n",
    "    tokenized_corpus_rank = [tokenize(doc) for doc in corpus]\n",
    "\n",
    "    start = time.time()\n",
    "    bm25_rank = BM25Okapi(tokenized_corpus_rank)\n",
    "    index_time = time.time() - start\n",
    "\n",
    "    start_query = time.time()\n",
    "    for query in test_queries:\n",
    "        query_tokens = tokenize(query)\n",
    "        scores = bm25_rank.get_scores(query_tokens)\n",
    "        top_k = np.argsort(scores)[-10:][::-1]\n",
    "\n",
    "    query_time = time.time() - start_query\n",
    "    qps = len(test_queries) / query_time\n",
    "\n",
    "    print(f\"Indexing: {index_time:.4f}s\")\n",
    "    print(f\"Query time: {query_time:.4f}s\")\n",
    "    print(f\"QPS: {qps:.2f}\\n\")\n",
    "\n",
    "    return index_time, query_time, qps\n",
    "\n",
    "\n",
    "benchmark_rank_bm25()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "Kclp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 5. BM25S ===\n",
      "Indexing: 0.5329s\n",
      "Query time: 0.7049s\n",
      "QPS: 1418.74\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5328564643859863, 0.7048521041870117, 1418.7373408687158)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def benchmark_bm25s():\n",
    "    import bm25s\n",
    "\n",
    "    print(\"=== 5. BM25S ===\")\n",
    "\n",
    "    start = time.time()\n",
    "    retriever_bm25s = bm25s.BM25()\n",
    "    corpus_tokens_bm25s = bm25s.tokenize(corpus, show_progress=False)\n",
    "    retriever_bm25s.index(corpus_tokens_bm25s, show_progress=False)\n",
    "    index_time = time.time() - start\n",
    "\n",
    "    start_query = time.time()\n",
    "    for query in test_queries:\n",
    "        query_tokens_bm25s = bm25s.tokenize(query, show_progress=False)\n",
    "        results, scores = retriever_bm25s.retrieve(\n",
    "            query_tokens_bm25s, k=10, show_progress=False\n",
    "        )\n",
    "\n",
    "    query_time = time.time() - start_query\n",
    "    qps = len(test_queries) / query_time\n",
    "\n",
    "    print(f\"Indexing: {index_time:.4f}s\")\n",
    "    print(f\"Query time: {query_time:.4f}s\")\n",
    "    print(f\"QPS: {qps:.2f}\\n\")\n",
    "\n",
    "    return index_time, query_time, qps\n",
    "\n",
    "\n",
    "benchmark_bm25s()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "emfo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 6. POLARS OPTIMIZATION ===\n",
      "Indexing: 0.1580s\n",
      "Query time: 0.6143s\n",
      "QPS: 1627.94\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1579906940460205, 0.6142714023590088, 1627.9449053946898)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def benchmark_polars():\n",
    "    import polars as pl\n",
    "\n",
    "    print(\"=== 6. POLARS OPTIMIZATION ===\")\n",
    "    start = time.time()\n",
    "\n",
    "    k1 = 1.5\n",
    "    b = 0.75\n",
    "    N = len(corpus)\n",
    "\n",
    "    # Create LazyFrame\n",
    "    lf = pl.LazyFrame({\"doc_id\": range(N), \"text\": corpus})\n",
    "\n",
    "    # Tokenize and explode\n",
    "    lf_tokens = (\n",
    "        lf.with_columns(\n",
    "            pl.col(\"text\").str.to_lowercase().str.split(\" \").alias(\"tokens\")\n",
    "        )\n",
    "        .explode(\"tokens\")\n",
    "        .filter(pl.col(\"tokens\").str.len_chars() > 0)\n",
    "    )\n",
    "\n",
    "    # Build vocab\n",
    "    vocab_df = (\n",
    "        lf_tokens.select(pl.col(\"tokens\").unique()).collect().with_row_index(\"word_idx\")\n",
    "    )\n",
    "    vocab_size = len(vocab_df)\n",
    "\n",
    "    # Join to map tokens to indices\n",
    "    lf_tokens = lf_tokens.join(vocab_df.lazy(), on=\"tokens\", how=\"left\")\n",
    "\n",
    "    # Compute TF\n",
    "    lf_tf = lf_tokens.group_by([\"doc_id\", \"word_idx\"]).agg(pl.len().alias(\"tf\"))\n",
    "\n",
    "    # Compute document lengths\n",
    "    lf_doc_lens = lf_tf.group_by(\"doc_id\").agg(pl.col(\"tf\").sum().alias(\"doc_len\"))\n",
    "    avg_doc_len_pl = lf_doc_lens.select(pl.col(\"doc_len\").mean()).collect().item()\n",
    "\n",
    "    # Compute DF and IDF\n",
    "    lf_idf = (\n",
    "        lf_tf.group_by(\"word_idx\")\n",
    "        .agg(pl.col(\"doc_id\").n_unique().alias(\"df\"))\n",
    "        .with_columns(\n",
    "            ((pl.lit(N) - pl.col(\"df\") + 0.5) / (pl.col(\"df\") + 0.5)).log().alias(\"idf\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Calculate BM25\n",
    "    df_bm25_pl = (\n",
    "        lf_tf.join(lf_doc_lens, on=\"doc_id\", how=\"left\")\n",
    "        .join(lf_idf, on=\"word_idx\", how=\"left\")\n",
    "        .with_columns(\n",
    "            (\n",
    "                pl.col(\"idf\")\n",
    "                * pl.col(\"tf\")\n",
    "                * (k1 + 1)\n",
    "                / (\n",
    "                    pl.col(\"tf\")\n",
    "                    + k1 * (1 - b + b * (pl.col(\"doc_len\") / avg_doc_len_pl))\n",
    "                )\n",
    "            ).alias(\"bm25_score\")\n",
    "        )\n",
    "        .select([\"doc_id\", \"word_idx\", \"bm25_score\"])\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    # Build sparse matrix\n",
    "    rows_pl = df_bm25_pl[\"word_idx\"].to_numpy()\n",
    "    cols_pl = df_bm25_pl[\"doc_id\"].to_numpy()\n",
    "    data_pl = df_bm25_pl[\"bm25_score\"].to_numpy()\n",
    "    word_to_idx_pl = dict(zip(vocab_df[\"tokens\"], vocab_df[\"word_idx\"]))\n",
    "    bm25_matrix_pl = csr_matrix((data_pl, (rows_pl, cols_pl)), shape=(vocab_size, N))\n",
    "\n",
    "    index_time = time.time() - start\n",
    "\n",
    "    # Query\n",
    "    start_query = time.time()\n",
    "    for query in test_queries:\n",
    "        query_indices = [\n",
    "            word_to_idx_pl[word] for word in tokenize(query) if word in word_to_idx_pl\n",
    "        ]\n",
    "        if query_indices:\n",
    "            doc_scores = np.array(\n",
    "                bm25_matrix_pl[query_indices, :].sum(axis=0)\n",
    "            ).flatten()\n",
    "            top_k = np.argsort(doc_scores)[-10:][::-1]\n",
    "\n",
    "    query_time = time.time() - start_query\n",
    "    qps = len(test_queries) / query_time\n",
    "\n",
    "    print(f\"Indexing: {index_time:.4f}s\")\n",
    "    print(f\"Query time: {query_time:.4f}s\")\n",
    "    print(f\"QPS: {qps:.2f}\\n\")\n",
    "\n",
    "    return index_time, query_time, qps\n",
    "\n",
    "\n",
    "benchmark_polars()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nWHF",
   "metadata": {},
   "source": [
    "## üèÜ BM25 Benchmark Results (8,530 documents, 1000 queries)\n",
    "\n",
    "| Implementation | Indexing (s) | Query (s) | QPS |\n",
    "|----------------|-------------|-----------|-----|\n",
    "| **1. Python Loops** | 0.0801 | 23.6460 | 42.29 |\n",
    "| **2. Sparse + Python Loops** | 77.9146 | 0.4709 | 2,123.64 |\n",
    "| **3. Sparse + NumPy** | 6.9885 | 0.5250 | 1,904.79 |\n",
    "| **4. rank-bm25** | 0.0813 | 5.0695 | 197.26 |\n",
    "| **5. bm25s** | 0.5329 | 0.7049 | 1,418.74 |\n",
    "| **6. Polars** | 0.1580 | 0.6143 | 1,627.94 |\n",
    "---\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "\n",
    "| Category | Winner | Value |\n",
    "|----------|--------|-------|\n",
    "| **Fastest Indexing** | Python Loops | 0.0801s |\n",
    "| **Fastest Query** | Sparse + Python Loops | 0.4709s |\n",
    "| **Highest QPS** | Sparse + Python Loops | 2,123.64 |\n",
    "| **Best Balance** | **Polars** | 0.1580s index, 1,627.94 QPS |\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "\n",
    "1. [bm25 part 1](https://kareemai.com/blog/posts/nlp/embedding_world/sparse_embedding/bm25_from_scratch.html)\n",
    "2. [bm25 part 2](https://kareemai.com/blog/posts/nlp/embedding_world/sparse_embedding/bm25_arabic_qdrant.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6754d999",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}