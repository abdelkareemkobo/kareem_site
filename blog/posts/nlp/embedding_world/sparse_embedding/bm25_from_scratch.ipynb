{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b49521fe",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"BM25 Explained for Hybrid Search and Rag | Part 1\"\n",
    "description: \"BM25 explained with python code implementation and math examples \"\n",
    "author: \"kareem\"\n",
    "date: 2025-12-19\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "    code-tools: true\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "categories:\n",
    "    - blogging\n",
    "    - embedding\n",
    "    - qdrant \n",
    "image: \"https://kareemai.com/blog/posts/nlp/embedding_world/sparse_embedding/images/bm25.png\"\n",
    "open-graph:\n",
    "  image: \"https://kareemai.com/blog/posts/nlp/embedding_world/sparse_embedding/images/bm25.png\"\n",
    "twitter-card:\n",
    "  image: \"https://kareemai.com/blog/posts/nlp/embedding_world/sparse_embedding/images/bm25.png\"\n",
    "draft: false\n",
    "execute: \n",
    "    echo: true\n",
    "jupyter: python3\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e4c91",
   "metadata": {},
   "source": [
    "\n",
    "## BM25 Explained \n",
    "\n",
    "Implementing BM25 Search Algorithm from Scratch\n",
    "\n",
    "BM25 (Best Matching 25) is a ranking function used by search engines to estimate the relevance of documents to a given search query. \n",
    "\n",
    "It's an improvement over TF-IDF that handles term frequency saturation and document length normalization.\n",
    "\n",
    "****\n",
    "\n",
    "**you will find a marimo version in the references that will help you understand the equation better.**\n",
    "\n",
    "![marimo bm25](images/bm_25_marimo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af13e01",
   "metadata": {},
   "source": [
    "## 1. Sample Documents\n",
    "\n",
    "Let's start with a small collection of documents to search through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eab7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Python is a programming language\",\n",
    "    \"I love Python programming\",\n",
    "    \"Java is also a programming language\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f95261",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "First, we need to break text into individual words (tokens). We'll convert to lowercase for case-insensitive matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d0f48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: ['python', 'is', 'a', 'programming', 'language']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Convert text to lowercase and split into words.\"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "# Test tokenization\n",
    "print(\"Example:\", tokenize(documents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699227a4",
   "metadata": {},
   "source": [
    "## 3. Term Frequency (TF)\n",
    "\n",
    "For each document, we count how many times each word appears. \n",
    "\n",
    "We'll use Python's `Counter` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f85d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0: {'python': 1, 'is': 1, 'a': 1, 'programming': 1, 'language': 1}\n",
      "Document 1: {'i': 1, 'love': 1, 'python': 1, 'programming': 1}\n",
      "Document 2: {'java': 1, 'is': 1, 'also': 1, 'a': 1, 'programming': 1, 'language': 1}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def compute_term_frequencies(documents):\n",
    "    \"\"\"Compute term frequency for each document.\"\"\"\n",
    "    doc_term_freqs = []\n",
    "    for doc in documents:\n",
    "        tokens = tokenize(doc)\n",
    "        term_freq = Counter(tokens)\n",
    "        doc_term_freqs.append(term_freq)\n",
    "    return doc_term_freqs\n",
    "\n",
    "\n",
    "all_docs_terms = compute_term_frequencies(documents)\n",
    "\n",
    "# Display term frequencies for each document\n",
    "for i, tf in enumerate(all_docs_terms):\n",
    "    print(f\"Document {i}: {dict(tf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471ef60f",
   "metadata": {},
   "source": [
    "## 4. Document Frequency (DF)\n",
    "\n",
    "Document frequency counts in how many documents each term appears. This helps identify common vs. rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea190db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Frequencies: {'python': 2, 'is': 2, 'a': 2, 'programming': 3, 'language': 2, 'i': 1, 'love': 1, 'java': 1, 'also': 1}\n"
     ]
    }
   ],
   "source": [
    "def compute_document_frequency(doc_term_freqs):\n",
    "    \"\"\"Count how many documents each term appears in.\"\"\"\n",
    "    df = {}\n",
    "    for doc_tf in doc_term_freqs:\n",
    "        for term in doc_tf.keys():\n",
    "            df[term] = df.get(term, 0) + 1\n",
    "    return df\n",
    "\n",
    "\n",
    "df = compute_document_frequency(all_docs_terms)\n",
    "print(\"Document Frequencies:\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c44bdf",
   "metadata": {},
   "source": [
    "## 5. Inverse Document Frequency (IDF)\n",
    "\n",
    "IDF measures how rare or common a word is across all documents. Rare words get higher scores.\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$\\text{IDF}(w) = \\log\\left(\\frac{N - \\text{DF}(w) + 0.5}{\\text{DF}(w) + 0.5}\\right)$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $N$ = total number of documents\n",
    "\n",
    "- $\\text{DF}(w)$ = document frequency of word $w$\n",
    "\n",
    "### Why use Logarthim in IDF? \n",
    " \n",
    "The logarithm **compresses the scale** of scores. Without it:\n",
    "\n",
    "- A word appearing in 1 out of 10,000 documents would dominate everything\n",
    "\n",
    "- Rare words would have scores thousands of times higher than slightly less rare words \n",
    "\n",
    "The log smooths this out so differences are more reasonable. \n",
    "\n",
    "\n",
    "The `+0.5` terms are a **smoothing trick** to handle edge cases (like when a word appears in all or no documents).\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd5487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF('python'): -0.5108\n",
      "IDF('java'): 0.5108\n",
      "IDF('programming'): -1.9459\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def compute_idf(term, df, num_docs):\n",
    "    \"\"\"Calculate IDF score for a term.\"\"\"\n",
    "    if term not in df:\n",
    "        return 0\n",
    "    df_term = df[term]\n",
    "    return math.log((num_docs - df_term + 0.5) / (df_term + 0.5))\n",
    "\n",
    "\n",
    "# Test IDF scores\n",
    "print(f\"IDF('python'): {compute_idf('python', df, len(documents)):.4f}\")\n",
    "print(f\"IDF('java'): {compute_idf('java', df, len(documents)):.4f}\")\n",
    "print(f\"IDF('programming'): {compute_idf('programming', df, len(documents)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5b884c",
   "metadata": {},
   "source": [
    "### Understanding Negative vs Positive IDF\n",
    "\n",
    "| Word | Appears in | IDF | Meaning |\n",
    "|------|------------|-----|---------|\n",
    "| \"java\" | 1/3 docs | +0.51 | Rare → discriminating → useful |\n",
    "| \"python\" | 2/3 docs | -0.51 | Common → less useful |\n",
    "| \"programming\" | 3/3 docs | -1.10 | Very common → penalized |\n",
    "\n",
    "**Key insight:** Words that appear in more than half the documents get negative IDF scores. \n",
    "\n",
    "They hurt relevance because they don't help distinguish documents!\n",
    "\n",
    "### Why Do Some Scores Equal Zero?\n",
    "\n",
    "When query terms have opposite IDF values, they can cancel out:\n",
    "\n",
    "For query `\"love python\"` in Document 1:\n",
    "\n",
    "\n",
    "- \"love\" IDF = +0.51 (rare, appears in 1 doc)\n",
    "\n",
    "- \"python\" IDF = -0.51 (common, appears in 2 docs)\n",
    "\n",
    "- Total ≈ 0\n",
    "\n",
    "This is a limitation of small document collections. \n",
    "\n",
    "With thousands of documents, rare words would have much higher positive scores and wouldn't be canceled out.\n",
    "\n",
    "### Why Does \"Java\" Score Better Than \"Python\"?\n",
    "\n",
    "\n",
    "For query `\"java programming\"`:\n",
    "\n",
    "- \"java\" has positive IDF (+0.51) because it's rare\n",
    "\n",
    "- \"programming\" has negative IDF (-1.10) because it's everywhere\n",
    "\n",
    "For query `\"python programming\"`:\n",
    "\n",
    "- \"python\" has negative IDF (-0.51) \n",
    "\n",
    "- \"programming\" has negative IDF (-1.10)\n",
    "\n",
    "- Both terms are negative → all documents score poorly!\n",
    "\n",
    "**Takeaway:** BM25 rewards queries containing rare, discriminating terms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7361f",
   "metadata": {},
   "source": [
    "## 6. Average Document Length\n",
    "\n",
    "BM25 normalizes scores by document length to avoid bias toward longer documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c0e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average document length: 5.00 unique terms\n"
     ]
    }
   ],
   "source": [
    "def compute_avg_doc_length(doc_term_freqs):\n",
    "    \"\"\"Calculate average document length (in terms).\"\"\"\n",
    "    total_length = sum(len(doc_tf) for doc_tf in doc_term_freqs)\n",
    "    return total_length / len(doc_term_freqs)\n",
    "\n",
    "\n",
    "avg_doc_len = compute_avg_doc_length(all_docs_terms)\n",
    "print(f\"Average document length: {avg_doc_len:.2f} unique terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1247db",
   "metadata": {},
   "source": [
    "## 7. BM25 Score for a Single Term\n",
    "\n",
    "The BM25 score for a single term in a document is:\n",
    "\n",
    "$$\\text{BM25}(w, d) = \\text{IDF}(w) \\cdot \\frac{\\text{TF}(w, d) \\cdot (k_1 + 1)}{\\text{TF}(w, d) + k_1 \\cdot \\left(1 - b + b \\cdot \\frac{|d|}{\\text{avgdl}}\\right)}$$\n",
    "\n",
    "where:\n",
    "\n",
    "\n",
    "- $\\text{TF}(w, d)$ = term frequency of word $w$ in document $d$\n",
    "\n",
    "- $|d|$ = length of document $d$\n",
    "\n",
    "- $\\text{avgdl}$ = average document length\n",
    "\n",
    "- $k_1$ = term frequency saturation parameter (typically 1.2-2.0)\n",
    "\n",
    "- $b$ = length normalization parameter (typically 0.75)\n",
    "\n",
    "### The Role of Parameters $k_1$ and $b$\n",
    "\n",
    "| Parameter | Controls | Low Value | High Value |\n",
    "|-----------|----------|-----------|------------|\n",
    "| $k_1$ (1.2-2.0) | Term frequency saturation | TF matters less | TF matters more |\n",
    "| $b$ (0-1) | Length normalization | Ignore length | Penalize long docs |\n",
    "\n",
    "- $k_1 = 0$: Only IDF matters, TF is ignored\n",
    "\n",
    "- $b = 0$: Document length is ignored\n",
    "\n",
    "- $b = 1$: Full length normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4186497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_term_score(term, doc_tf, idf_score, doc_length, avg_doc_len, k1=1.5, b=0.75):\n",
    "    \"\"\"Calculate BM25 score for a single term in a document.\"\"\"\n",
    "    tf = doc_tf.get(term, 0)\n",
    "    if tf == 0:\n",
    "        return 0\n",
    "\n",
    "    numerator = idf_score * tf * (k1 + 1)\n",
    "    denominator = tf + k1 * (1 - b + b * (doc_length / avg_doc_len))\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6e7590",
   "metadata": {},
   "source": [
    "## 8. Full BM25 Search\n",
    "\n",
    "To score a query against all documents:\n",
    "\n",
    "1. Tokenize the query\n",
    "\n",
    "2. For each document, sum the BM25 scores of all query terms\n",
    "\n",
    "3. Return scores for all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63dbf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_search(query, documents, doc_term_freqs, df, avg_doc_len, k1=1.5, b=0.75):\n",
    "    \"\"\"\n",
    "    Score all documents for a given query using BM25.\n",
    "\n",
    "    Returns a list of (document_index, score) tuples sorted by relevance.\n",
    "    \"\"\"\n",
    "    query_tokens = tokenize(query)\n",
    "    num_docs = len(documents)\n",
    "    scores = []\n",
    "\n",
    "    for doc_idx, doc_tf in enumerate(doc_term_freqs):\n",
    "        doc_length = sum(doc_tf.values())  # Total terms in document\n",
    "        doc_score = 0\n",
    "\n",
    "        for term in query_tokens:\n",
    "            idf_score = compute_idf(term, df, num_docs)\n",
    "            doc_score += bm25_term_score(\n",
    "                term, doc_tf, idf_score, doc_length, avg_doc_len, k1, b\n",
    "            )\n",
    "\n",
    "        scores.append((doc_idx, doc_score))\n",
    "\n",
    "    # Sort by score (descending)\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823280bc",
   "metadata": {},
   "source": [
    "## 9. Test the Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f2ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'python programming'\n",
      "------------------------------------------------------------\n",
      "1. [Score: -1.785] Java is also a programming language\n",
      "2. [Score: -2.457] Python is a programming language\n",
      "3. [Score: -2.700] I love Python programming\n",
      "\n",
      "Query: 'java programming'\n",
      "------------------------------------------------------------\n",
      "1. [Score: -1.317] Java is also a programming language\n",
      "2. [Score: -1.946] Python is a programming language\n",
      "3. [Score: -2.138] I love Python programming\n",
      "\n",
      "Query: 'love python'\n",
      "------------------------------------------------------------\n",
      "1. [Score:  0.000] I love Python programming\n",
      "2. [Score:  0.000] Java is also a programming language\n",
      "3. [Score: -0.511] Python is a programming language\n"
     ]
    }
   ],
   "source": [
    "def display_results(query, results, documents):\n",
    "    \"\"\"Pretty print search results.\"\"\"\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"-\" * 60)\n",
    "    for rank, (doc_idx, score) in enumerate(results, 1):\n",
    "        print(f\"{rank}. [Score: {score:6.3f}] {documents[doc_idx]}\")\n",
    "\n",
    "\n",
    "# Test different queries\n",
    "queries = [\"python programming\", \"java programming\", \"love python\"]\n",
    "\n",
    "for query in queries:\n",
    "    results = bm25_search(query, documents, all_docs_terms, df, avg_doc_len)\n",
    "    display_results(query, results, documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae83c44",
   "metadata": {},
   "source": [
    "## 10. Understanding the Results\n",
    "\n",
    "- **Negative scores** can occur when query terms appear in most documents (high DF)\n",
    "- **Higher scores** indicate better relevance\n",
    "- With small document collections, common words dominate; BM25 works best with larger collections\n",
    "- The parameters $k_1$ and $b$ can be tuned for different applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c9059",
   "metadata": {},
   "source": [
    "## The Three Ideas Behind BM25\n",
    "\n",
    "1. **Term Frequency:** Words appearing more often in a doc → more relevant\n",
    "   - BUT with diminishing returns (5 mentions isn't 5x better than 1)\n",
    "\n",
    "2. **Inverse Document Frequency:** Rare words matter more\n",
    "   - \"quantum\" is more useful than \"the\"\n",
    "\n",
    "3. **Document Length Normalization:** Shorter docs are often more focused\n",
    "   - A 100-word doc mentioning \"python\" once may be more relevant than a 10,000-word doc mentioning it once\n",
    "\n",
    "\n",
    "Here is [Part 2: BM25 with Qdrant](https://kareemai.com/blog/posts/nlp/embedding_world/sparse_embedding/bm25_arabic_qdrant.html): A real use case for using BM25 with Gemini embeddings to improve search results for real estate.\n",
    "\n",
    "Also the [Marimo BM25 Explained](https://molab.marimo.io/notebooks/nb_qmRNbuWUz4fvLdfDt8Un7E/app)\n",
    "\n",
    "A benchmark: [BM25 Benchmark](https://kareemai.com/blog/posts/nlp/embedding_world/sparse_embedding/bm25_benchmark_full.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b63a9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
